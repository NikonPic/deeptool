{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.rnnvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from deeptool.architecture import Encoder, Decoder, DownUpConv\n",
    "from deeptool.utils import Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN VAE\n",
    "\n",
    "> Structure for an Approach maintained a pseudo space realtion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load some test dataset to confirm architecture:\n",
    "from deeptool.parameters import get_all_args\n",
    "from deeptool.dataloader import load_test_batch\n",
    "args = get_all_args()\n",
    "args.model_type = \"rnnvae\"\n",
    "args.batch_size = 1\n",
    "batch = load_test_batch(args)\n",
    "batch[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def mod_batch(batch, key=\"img\"):\n",
    "    \"\"\"\n",
    "    transform the batch to be compatible with the network by permuting\n",
    "    \"\"\"\n",
    "    batch[key] = batch[key][0, :, :, :, :]\n",
    "    batch[key] = batch[key].permute(1, 0, 2, 3)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = mod_batch(batch)\n",
    "batch[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dim = 2\n",
    "enc_part = DownUpConv(args, pic_size=256, n_fea_in=3, n_fea_next=8, depth=1, )\n",
    "enc_part.min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, device, args):\n",
    "        \"\"\"\n",
    "        The recurrent autoencoder for compressing 3d data.\n",
    "        It compresses in 2d while (hopefully) maintaining the spatial relation between layers\n",
    "        \"\"\"\n",
    "        super(RNN_VAE, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # 1. create the convolutional Encoder\n",
    "        self.conv_part_enc = DownUpConv(args, pic_size=args.pic_size, n_fea_in=len(\n",
    "            args.perspectives), n_fea_next=args.n_fea_up, depth=1).to(self.device)\n",
    "\n",
    "        # save important features\n",
    "        max_fea, min_size = self.conv_part_enc.max_fea, self.conv_part_enc.min_size\n",
    "        self.view_arr = [-1, max_fea * min_size**2]\n",
    "\n",
    "        # 2. Apply FC- Encoder Part\n",
    "        self.fc_part_enc = nn.Sequential(\n",
    "            nn.Linear(max_fea*min_size*min_size, max_fea*min_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(max_fea*min_size, max_fea),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(max_fea, args.n_z),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # 3. Transition Layer: GRU\n",
    "        self.transition = nn.GRU(args.n_z, args.n_z, 1).to(self.device)\n",
    "\n",
    "        # 4. Apply FC-Decoder Part\n",
    "        self.fc_part_dec = nn.Sequential(\n",
    "            nn.Linear(args.n_z, max_fea),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(max_fea, max_fea*min_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(max_fea*min_size, max_fea*min_size*min_size),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # 5. create the convolutional Decoder\n",
    "        self.conv_part_dec = DownUpConv(\n",
    "            args, pic_size=args.pic_size, n_fea_in=len(\n",
    "            args.perspectives), n_fea_next=args.n_fea_down, depth=1, move='up').to(self.device)\n",
    "    \n",
    "    def rnn_transition(self, x):\n",
    "        \"\"\"\n",
    "        take the matrix of encoded input slices and apply the RNN part\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        calculate the forward pass\n",
    "        \"\"\"\n",
    "        # move to gpu\n",
    "        x = batch['img'].to(self.device)\n",
    "        \n",
    "        # encode:\n",
    "        x = self.conv_part_enc(x)\n",
    "        x = x.reshape(self.view_arr)\n",
    "        x = self.fc_part_enc(x)\n",
    "        \n",
    "        # apply the GRU transition\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (\n",
    "        torch.cuda.is_available() and args.n_gpu > 0) else \"cpu\")\n",
    "rnn_vae = RNN_VAE(device, args)\n",
    "rnn_vae(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataloader.ipynb.\n",
      "Converted 01_architecture.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_parameters.ipynb.\n",
      "Converted 04_train_loop.ipynb.\n",
      "Converted 10_diagnosis.ipynb.\n",
      "Converted 20_dcgan.ipynb.\n",
      "Converted 21_introvae.ipynb.\n",
      "Converted 22_vqvae.ipynb.\n",
      "Converted 23_rnn_vae.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? DownUpConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "82fdadc0-b84a-48d1-9b8e-d8a208951284"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
