{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.rnnvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from deeptool.architecture import Encoder, Decoder, DownUpConv\n",
    "from deeptool.utils import Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN VAE\n",
    "\n",
    "> Structure for an Approach maintained a pseudo space realtion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load some test dataset to confirm architecture:\n",
    "from deeptool.parameters import get_all_args\n",
    "from deeptool.dataloader import load_test_batch\n",
    "args = get_all_args()\n",
    "args.model_type = \"rnnvae\"\n",
    "args.batch_size = 1\n",
    "args.track = False\n",
    "batch = load_test_batch(args)\n",
    "batch[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def mod_batch(batch, key=\"img\"):\n",
    "    \"\"\"\n",
    "    transform the batch to be compatible with the network by permuting\n",
    "    \"\"\"\n",
    "    if len(batch[key].shape) > 4:\n",
    "        batch[key] = batch[key][0, :, :, :, :]\n",
    "        batch[key] = batch[key].permute(1, 0, 2, 3)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchmod = mod_batch(batch)\n",
    "batchmod[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class RNN_VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, device, args):\n",
    "        \"\"\"\n",
    "        The recurrent autoencoder for compressing 3d data.\n",
    "        It compresses in 2d while (hopefully) maintaining the spatial relation between layers\n",
    "        \"\"\"\n",
    "        super(RNN_VAE, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # 1. create the convolutional Encoder\n",
    "        args.dim = 2\n",
    "        self.conv_part_enc = DownUpConv(args, pic_size=args.pic_size, n_fea_in=len(\n",
    "            args.perspectives), n_fea_next=args.n_fea_up, depth=1).to(self.device)\n",
    "\n",
    "        # save important features\n",
    "        max_fea, min_size = self.conv_part_enc.max_fea, self.conv_part_enc.min_size\n",
    "        self.n_z = args.n_z\n",
    "\n",
    "        self.view_arr = [-1, max_fea * min_size**2]  # as flat vector\n",
    "        self.view_conv = [-1, max_fea, min_size, min_size]  # as conv block\n",
    "        self.view_track = [1, len(args.perspectives), -1,\n",
    "                           args.pic_size, args.pic_size]\n",
    "\n",
    "        # 2. Apply FC- Encoder Part\n",
    "        self.fc_part_enc = nn.Sequential(\n",
    "            nn.Linear(max_fea*min_size*min_size, max_fea*min_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(max_fea*min_size, max_fea),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(max_fea, args.n_z),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # 3. Transition Layer: GRU\n",
    "        self.transition = nn.GRU(args.n_z, args.n_z, 1).to(self.device)\n",
    "\n",
    "        # 4. Apply FC-Decoder Part\n",
    "        self.fc_part_dec = nn.Sequential(\n",
    "            nn.Linear(args.n_z, max_fea),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(max_fea, max_fea*min_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(max_fea*min_size, max_fea*min_size*min_size),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # 5. create the convolutional Decoder\n",
    "        self.conv_part_dec = DownUpConv(\n",
    "            args, pic_size=args.pic_size, n_fea_in=len(\n",
    "                args.perspectives), n_fea_next=args.n_fea_down, depth=1, move='up').to(self.device)\n",
    "\n",
    "        # the standard loss\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        # the optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=args.lr)\n",
    "\n",
    "        # reset the dimension\n",
    "        args.dim = 3\n",
    "\n",
    "        # Setup the tracker to visualize the progress\n",
    "        if args.track:\n",
    "            self.tracker = Tracker(args)\n",
    "\n",
    "    def watch_progress(self, test_data, iteration):\n",
    "        \"\"\"Outsourced to Tracker\"\"\"\n",
    "        self.tracker.track_progress(self, test_data, iteration)\n",
    "\n",
    "    def rnn_transition(self, x):\n",
    "        \"\"\"\n",
    "        take the matrix of encoded input slices and apply the RNN part\n",
    "        \"\"\"\n",
    "        # reshape\n",
    "        x = x.reshape([-1, 1, self.n_z])\n",
    "        # apply GRU layer\n",
    "        x, _ = self.transition(x)\n",
    "        # reshape\n",
    "        x = x.reshape([-1, self.n_z])\n",
    "        return x\n",
    "\n",
    "    def forward(self, batch, update=True):\n",
    "        \"\"\"\n",
    "        calculate the forward pass\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "\n",
    "        batch = mod_batch(batch)\n",
    "\n",
    "        # move to gpu\n",
    "        img = batch['img'].to(self.device)\n",
    "\n",
    "        # encode:\n",
    "        x = self.conv_part_enc(img)\n",
    "        x = x.reshape(self.view_arr)\n",
    "        x = self.fc_part_enc(x)\n",
    "\n",
    "        # apply the GRU transition\n",
    "        x = self.rnn_transition(x)\n",
    "\n",
    "        # decode\n",
    "        x = self.fc_part_dec(x)\n",
    "        x = x.reshape(self.view_conv)\n",
    "        x = self.conv_part_dec(x)\n",
    "\n",
    "        loss = self.mse_loss(img, x)\n",
    "\n",
    "        #x = x.reshape(self.view_track)\n",
    "\n",
    "        if update:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            return x\n",
    "\n",
    "        else:\n",
    "            tr_data = {}\n",
    "            tr_data[\"loss\"] = loss.item()\n",
    "\n",
    "        return x, tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (\n",
    "        torch.cuda.is_available() and args.n_gpu > 0) else \"cpu\")\n",
    "rnn_vae = RNN_VAE(device, args)\n",
    "data = load_test_batch(args)\n",
    "x, tr = rnn_vae(data, update=False)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = load_test_batch(args)\n",
    "batch[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from deeptool.train_loop import test_one_batch\n",
    "from deeptool.parameters import get_all_args, compat_args\n",
    "args = get_all_args()\n",
    "args.pic_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 dim test\n",
    "args.model_type = \"rnnvae\"\n",
    "args.dim = 3\n",
    "args = compat_args(args)\n",
    "test_one_batch(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataloader.ipynb.\n",
      "Converted 01_architecture.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_parameters.ipynb.\n",
      "Converted 04_train_loop.ipynb.\n",
      "Converted 10_diagnosis.ipynb.\n",
      "Converted 20_dcgan.ipynb.\n",
      "Converted 21_introvae.ipynb.\n",
      "Converted 22_vqvae.ipynb.\n",
      "Converted 23_rnn_vae.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "82fdadc0-b84a-48d1-9b8e-d8a208951284"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
