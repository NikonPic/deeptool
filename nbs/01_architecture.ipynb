{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "> Defining the general Architectures for 3D Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Init function and single Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolving Normalization-Activation Layers\n",
    "\n",
    "https://arxiv.org/pdf/2004.02967.pdf\n",
    "inspired by : https://github.com/digantamisra98/EvoNorm\n",
    "\n",
    "<img src=\"img/evo.png\" alt=\"Drawing\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instance_std(x, eps=1e-5):\n",
    "    var = torch.var(x, dim=(2, 3), keepdim=True).expand_as(x)\n",
    "    return torch.sqrt(var + eps)\n",
    "\n",
    "\n",
    "def group_std(x, groups=32, eps=1e-5):\n",
    "    N, C, H, W = x.size()\n",
    "    groups = C if groups > C else groups\n",
    "    x = torch.reshape(x, (N, groups, C // groups, H, W))\n",
    "    var = torch.var(x, dim=(2, 3, 4), keepdim=True).expand_as(x)\n",
    "    return torch.reshape(torch.sqrt(var + eps), (N, C, H, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EvoNorm2D(nn.Module):\n",
    "    def __init__(\n",
    "        self, input, non_linear=True, version=\"S0\", momentum=0.9, training=True\n",
    "    ):\n",
    "        super(EvoNorm2D, self).__init__()\n",
    "        self.non_linear = non_linear\n",
    "        self.version = version\n",
    "        self.training = training\n",
    "        self.momentum = momentum\n",
    "        if self.version not in [\"B0\", \"S0\"]:\n",
    "            raise ValueError(\"Invalid EvoNorm version\")\n",
    "        self.insize = input\n",
    "        self.gamma = nn.Parameter(torch.ones(1, self.insize, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, self.insize, 1, 1))\n",
    "        if self.non_linear:\n",
    "            self.v = nn.Parameter(torch.ones(1, self.insize, 1, 1))\n",
    "        self.register_buffer(\"running_var\", torch.ones(1, self.insize, 1, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # prepare the functionality in advance\n",
    "        if self.version == \"S0\":\n",
    "            self.forward = self.forward_S0_nl if self.non_linear else self.forward_S0_l\n",
    "\n",
    "        if self.version == \"B0\":\n",
    "            self.forward = self.forward_B0\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.running_var.fill_(1)\n",
    "\n",
    "    def forward_S0_nl(self, x):\n",
    "        num = x * torch.sigmoid(self.v * x)\n",
    "        return num / group_std(x) * self.gamma + self.beta\n",
    "\n",
    "    def forward_S0_l(self, x):\n",
    "        return x * self.gamma + self.beta\n",
    "\n",
    "    def forward_B0_nl(self, x):\n",
    "        if self.training:\n",
    "            var = torch.var(x, dim=(0, 2, 3), unbiased=False, keepdim=True).reshape(\n",
    "                1, x.size(1), 1, 1\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                self.running_var.copy_(\n",
    "                    self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "                )\n",
    "        else:\n",
    "            var = self.running_var\n",
    "\n",
    "        if self.non_linear:\n",
    "            den = torch.max((var + self.eps).sqrt(), self.v * x + instance_std(x))\n",
    "            return x / den * self.gamma + self.beta\n",
    "        else:\n",
    "            return x * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Define the weight parameters depending on the type:\n",
    "    Conv or Batchnorm\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Quantize(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantization 'Layer'\n",
    "    inspired by: https://github.com/deepmind/sonnet\n",
    "    modified from: https://github.com/rosinality/vq-vae-2-pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Setup the embedding Matrix (dim x n_embed)\n",
    "        \"\"\"\n",
    "        # This is equal to the feature number at the corresponding layer\n",
    "        self.dim = dim\n",
    "        # This is the discretization level for each pixel\n",
    "        self.n_embed = n_embed\n",
    "        # Learning parameters\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        # Init matrix of available embeddings\n",
    "        embed = torch.randn(dim, n_embed)\n",
    "        # Register to avoid gradients\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    def forward(self, x, update=True):\n",
    "        \"\"\"\n",
    "        Apply the Quantization on the input:\n",
    "        -> Update Embeddings\n",
    "        -> Remain the Gradient flow\n",
    "        Return:\n",
    "            Quantized input\n",
    "            Difference between true and\n",
    "        \"\"\"\n",
    "        # Reshape input\n",
    "        flatten = x.reshape(-1, self.dim)\n",
    "        # Calculate L2-distance between each pixel / voxel and embedding\n",
    "        dist = (\n",
    "            flatten.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * flatten @ self.embed\n",
    "            + self.embed.pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "        # Select the closest pairs\n",
    "        _, embed_ind = (-dist).max(1)\n",
    "        # Construct matrix of matching pairs\n",
    "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
    "        # Select the correct dimensions here!\n",
    "        embed_ind = embed_ind.view(*x.shape[:-1])\n",
    "        # Apply quantization\n",
    "        quantize = self.embed_code(embed_ind)\n",
    "\n",
    "        # Only for training -> Update Embeddings with moving average\n",
    "        if update:\n",
    "            # N_i = N_(i-1) * gamma + (1-gamma) * n_i\n",
    "            self.cluster_size.data.mul_(self.decay).add_(\n",
    "                1 - self.decay, embed_onehot.sum(0)\n",
    "            )\n",
    "            # Sum(E(x))\n",
    "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
    "            # m_i = m_(i-1) *gamma + (1-gamma) * Sum(E(x))\n",
    "            self.embed_avg.data.mul_(self.decay).add_(1 - self.decay, embed_sum)\n",
    "            # N_i\n",
    "            n = self.cluster_size.sum()\n",
    "            # norm N_i\n",
    "            cluster_size = (\n",
    "                (self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n\n",
    "            )\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
    "            # e_i = m_i / N_i\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        # Loss between Original to Quantization\n",
    "        diff = (quantize.detach() - x).pow(2).mean()\n",
    "        # Get Output, while enabling to copy gradients\n",
    "        quantize = x + (quantize - x).detach()\n",
    "        # Return results\n",
    "        return quantize, diff, embed_ind\n",
    "\n",
    "    def embed_code(self, embed_id):\n",
    "        \"\"\"\n",
    "        Perform the quantization by selecting all embedings from the ids\n",
    "        \"\"\"\n",
    "        return F.embedding(embed_id, self.embed.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16, 8, 64]) tensor(1.2985) torch.Size([16, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "quant = Quantize(64, 100)\n",
    "input = torch.randn(16, 16, 8, 64)\n",
    "quantize, diff, embed_ind = quant(input)\n",
    "print(quantize.shape, diff, embed_ind.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Block of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ResNet Block:\n",
    " \n",
    "<img src=\"img/resblock.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An individually designalble ResNet Block for 3 Dimensional Convoluions\n",
    "    based on: https://arxiv.org/abs/1512.03385\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_chan,\n",
    "        convsize=3,\n",
    "        activation=nn.ReLU(inplace=True),\n",
    "        init_w=weights_init,\n",
    "        dim=3,\n",
    "        evo_on=False,\n",
    "    ):\n",
    "        \"\"\"setup the general architecture\"\"\"\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        if dim == 3:\n",
    "            Conv = nn.Conv3d\n",
    "            BatchNorm = nn.BatchNorm3d\n",
    "            Evo = nn.BatchNorm3d\n",
    "\n",
    "        else:\n",
    "            Conv = nn.Conv2d\n",
    "            BatchNorm = nn.BatchNorm2d\n",
    "            Evo = EvoNorm2D if evo_on else nn.BatchNorm2d\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.conv1 = Conv(n_chan, n_chan, convsize, stride=1, padding=1, bias=False)\n",
    "        self.conv1.apply(init_w)\n",
    "\n",
    "        self.conv2 = Conv(n_chan, n_chan, convsize, stride=1, padding=1, bias=False)\n",
    "        self.conv2.apply(init_w)\n",
    "\n",
    "        self.bn1 = Evo(n_chan)\n",
    "        self.bn2 = Evo(n_chan)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the forward pass\"\"\"\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ConvBn(nn.Module):\n",
    "    \"\"\"\n",
    "    An individually designalble Block for 3 Dimensional Convoluions with Batchnorm and Dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chan,\n",
    "        out_chan,\n",
    "        convsize=3,\n",
    "        stride=2,\n",
    "        activation=nn.LeakyReLU(0.2, inplace=True),\n",
    "        init_w=weights_init,\n",
    "        padding=1,\n",
    "        dim=3,\n",
    "        p_drop=0,\n",
    "        evo_on=False\n",
    "    ):\n",
    "        \"\"\"setup the general architecture\"\"\"\n",
    "        super(ConvBn, self).__init__()\n",
    "\n",
    "        if dim == 3:\n",
    "            Conv = nn.Conv3d\n",
    "            BatchNorm = nn.BatchNorm3d\n",
    "            Dropout = nn.Dropout3d\n",
    "        else:\n",
    "            Conv = nn.Conv2d\n",
    "            BatchNorm = EvoNorm2D if evo_on else nn.BatchNorm2d\n",
    "            Dropout = nn.Dropout2d\n",
    "            # Check convsize and stride\n",
    "            if type(convsize) == tuple:\n",
    "                if len(convsize) > 2:\n",
    "                    convsize = convsize[1:]\n",
    "            if type(stride) == tuple:\n",
    "                if len(stride) > 2:\n",
    "                    stride = stride[1:]\n",
    "\n",
    "        self.main_part = nn.Sequential(\n",
    "            Conv(\n",
    "                in_chan, out_chan, convsize, stride=stride, padding=padding, bias=False\n",
    "            ),\n",
    "            BatchNorm(out_chan),\n",
    "            activation,\n",
    "            Dropout(p=p_drop),\n",
    "        )\n",
    "        self.main_part.apply(init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the forward pass\"\"\"\n",
    "        return self.main_part(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test it!\n",
    "m = ConvBn(1, 16, convsize=4, stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "inp = torch.randn(20, 1, 16, 64, 64)\n",
    "output = m(inp)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ConvTpBn(nn.Module):\n",
    "    \"\"\"\n",
    "    An individually designalble Block for 3 Dimensional Transposed Convoluions with Batchnorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chan,\n",
    "        out_chan,\n",
    "        convsize=3,\n",
    "        stride=2,\n",
    "        activation=nn.ReLU(inplace=True),\n",
    "        init_w=weights_init,\n",
    "        padding=1,\n",
    "        dim=3,\n",
    "        evo_on=False\n",
    "    ):\n",
    "        \"\"\"setup the general architecture\"\"\"\n",
    "        super(ConvTpBn, self).__init__()\n",
    "        if dim == 3:\n",
    "            ConvTranspose = nn.ConvTranspose3d\n",
    "            BatchNorm = nn.BatchNorm3d\n",
    "        else:\n",
    "            ConvTranspose = nn.ConvTranspose2d\n",
    "            BatchNorm = EvoNorm2D if evo_on else nn.BatchNorm2d\n",
    "            # Check convsize and stride\n",
    "            if type(convsize) == tuple:\n",
    "                if len(convsize) > 2:\n",
    "                    convsize = convsize[1:]\n",
    "            if type(stride) == tuple:\n",
    "                if len(stride) > 2:\n",
    "                    stride = stride[1:]\n",
    "\n",
    "        self.main_part = nn.Sequential(\n",
    "            ConvTranspose(\n",
    "                in_chan, out_chan, convsize, stride=stride, padding=padding, bias=False\n",
    "            ),\n",
    "            BatchNorm(out_chan),\n",
    "            activation,\n",
    "        )\n",
    "        self.main_part.apply(init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the forward pass\"\"\"\n",
    "        return self.main_part(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 31, 127, 127])\n"
     ]
    }
   ],
   "source": [
    "m = ConvTpBn(1, 16, convsize=3, stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "output = m(inp)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class LinearSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper class to provide a simple ending with linear layer and Sigmoid\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, y_dim, bias=False):\n",
    "        \"\"\"setup network\"\"\"\n",
    "        super(LinearSigmoid, self).__init__()\n",
    "        # store inputs\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.bias = bias\n",
    "        # make network\n",
    "        self.main_part = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.y_dim, bias=self.bias), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"reformat then apply fc part\"\"\"\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        x = self.main_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 The generic convolutional network block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DownUpConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Type, which contains the generic conv network\n",
    "    for 3d up- or downsclaing depending on \"move\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, args, n_fea_in, n_fea_next, pic_size, depth, move=\"down\", p_drop=0\n",
    "    ):\n",
    "        \"\"\"Setup the conv-network\"\"\"\n",
    "        super(DownUpConv, self).__init__()\n",
    "        # Convolutions in 2d / 3d\n",
    "        self.dim = args.dim\n",
    "        self.evo_on = args.evo_on\n",
    "        # Input dim\n",
    "        self.pic_size = pic_size\n",
    "        self.depth = depth\n",
    "        self.n_fea_in = n_fea_in\n",
    "        # Number of feature channels\n",
    "        self.n_fea_next = n_fea_next\n",
    "        # Output dim\n",
    "        self.min_size = args.min_size  # when to stop reducing\n",
    "        # Scaling\n",
    "        self.scale2d = args.scale2d\n",
    "        self.scale3d = args.scale3d\n",
    "        self.n_res2d = args.n_res2d\n",
    "        self.n_res3d = args.n_res3d\n",
    "        # Dropout\n",
    "        self.p_drop = p_drop\n",
    "        # Add the relevant quantization layers if required\n",
    "        self.vq_layers = args.vq_layers if args.model_type == \"vqvae\" else []\n",
    "        # Direction\n",
    "        self.move = move  # Define whether to scale up or down\n",
    "        self.main, output_tuple = self.generic_conv_init()\n",
    "        self.max_fea, self.max_fea_next, self.pic_out, self.final_depth = output_tuple\n",
    "\n",
    "    def add_layers(\n",
    "        self, conv_layers, fea_in, fea_out, n_res, pic_size, convsize=4, stride=2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add layers according to number of residual blocks and down/upscale mode\n",
    "        \"\"\"\n",
    "        # Downsample\n",
    "        # ---------------------------------------------------\n",
    "        if self.move == \"down\":\n",
    "            # 1. Downsample\n",
    "            conv_layers.extend(\n",
    "                [\n",
    "                    ConvBn(\n",
    "                        fea_in,\n",
    "                        fea_out,\n",
    "                        dim=self.dim,\n",
    "                        convsize=convsize,\n",
    "                        stride=stride,\n",
    "                        p_drop=self.p_drop,\n",
    "                        evo_on=self.evo_on\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            # 2. Add the residual blocks:\n",
    "            for _ in range(n_res):\n",
    "                conv_layers.extend(\n",
    "                    [\n",
    "                        ResNetBlock(\n",
    "                            fea_out, convsize=3, dim=self.dim, evo_on=self.evo_on,\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        # Upsample\n",
    "        # ---------------------------------------------------\n",
    "        else:\n",
    "            # Attention! This layer can be a quantization layer!\n",
    "            special_pic = pic_size / 2\n",
    "            if special_pic > self.min_size and special_pic in self.vq_layers:\n",
    "                fea_out *= 2  # so it has a doubled feature size\n",
    "\n",
    "            # 1. Add the residual blocks:\n",
    "            for _ in range(n_res):\n",
    "                conv_layers[:0] = [\n",
    "                    ResNetBlock(fea_in, convsize=3, dim=self.dim, evo_on=self.evo_on),\n",
    "                ]\n",
    "\n",
    "            # 2. Upsample\n",
    "            conv_layers[:0] = [\n",
    "                ConvTpBn(\n",
    "                    fea_out, fea_in, dim=self.dim, convsize=convsize, stride=stride, evo_on=self.evo_on,\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "    def generic_conv_init(self):\n",
    "        \"\"\"\n",
    "        Initialise the convolution layers generically:\n",
    "        Down:\n",
    "        Idea: -> Half picsize until all 3 dims are equal\n",
    "              -> Then reduce all dims until min_size.\n",
    "        Up:\n",
    "        Idea: -> Double all dims until z-limit is reached\n",
    "              -> Then double picsize until output-dim is reached.\n",
    "        \"\"\"\n",
    "\n",
    "        # Init the conv_layer list\n",
    "        conv_layers = []\n",
    "        # Current z-dim of the picture\n",
    "        cur_pic_dim = self.pic_size\n",
    "        # Current anz of features at input size\n",
    "        cur_fea_in = self.n_fea_in\n",
    "        # Current anz of features at output size\n",
    "        cur_fea_out = self.n_fea_next\n",
    "        # Current depth of the picture\n",
    "        cur_depth = self.depth\n",
    "        # Summarize current values\n",
    "        output_tuple = (cur_fea_in, cur_fea_out, cur_pic_dim, cur_depth)\n",
    "\n",
    "        # Until the limiting z_dim occurs or picsize too small\n",
    "        # ---------------------------------------------------------------\n",
    "        while cur_pic_dim > self.depth and cur_pic_dim > self.min_size:\n",
    "            # Add layers\n",
    "            self.add_layers(\n",
    "                conv_layers,\n",
    "                cur_fea_in,\n",
    "                cur_fea_out,\n",
    "                self.n_res2d,\n",
    "                cur_pic_dim,\n",
    "                convsize=(3, 4, 4),\n",
    "                stride=(1, 2, 2),\n",
    "            )\n",
    "            # Update input size\n",
    "            cur_fea_in = cur_fea_out\n",
    "            # Features are doupled\n",
    "            cur_fea_out *= self.scale2d\n",
    "            cur_pic_dim /= 2  # dimension is halved\n",
    "\n",
    "            # Store current values\n",
    "            output_tuple = (cur_fea_in, cur_fea_out, cur_pic_dim, cur_depth)\n",
    "\n",
    "            # CASE: Layer is a quantization layer!!\n",
    "            if cur_pic_dim in self.vq_layers:\n",
    "                # Return current Network and relevant parameters\n",
    "                return nn.Sequential(*conv_layers), output_tuple\n",
    "\n",
    "        # Limit reached, now continue until min-picsize reached\n",
    "        # ---------------------------------------------------------------\n",
    "        while cur_pic_dim > self.min_size:\n",
    "            # Add layers\n",
    "            self.add_layers(\n",
    "                conv_layers,\n",
    "                cur_fea_in,\n",
    "                cur_fea_out,\n",
    "                self.n_res3d,\n",
    "                cur_pic_dim,\n",
    "                convsize=4,\n",
    "                stride=2,\n",
    "            )\n",
    "            # Update input size\n",
    "            cur_fea_in = cur_fea_out\n",
    "            # Features are doupled\n",
    "            cur_fea_out *= self.scale3d\n",
    "            cur_pic_dim /= 2  # Dimension is halved\n",
    "            cur_depth /= 2  # Depth is also halfed\n",
    "\n",
    "            # Store current values\n",
    "            output_tuple = (cur_fea_in, cur_fea_out, cur_pic_dim, cur_depth)\n",
    "\n",
    "            # CASE: Layer is a quantization layer!!\n",
    "            if cur_pic_dim in self.vq_layers:\n",
    "                # Return current Network and relevant parameters\n",
    "                return nn.Sequential(*conv_layers), output_tuple\n",
    "\n",
    "        # Finally return the network at minimum size\n",
    "        return nn.Sequential(*conv_layers), output_tuple\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the forward pass\"\"\"\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Whole Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder with 3dimensional conv setup\"\"\"\n",
    "\n",
    "    def __init__(self, args, init_w=weights_init, vae_mode=True):\n",
    "        \"\"\"\n",
    "        Setup the Architecture:\n",
    "        Args:\n",
    "            ngpu = Number of GPUs available\n",
    "            init_w = Function for initialisation of weights\n",
    "            n_chan = Number of input channels: batch x n_chan x depth x size x size\n",
    "            n_d_fea = Number of feature channels within network\n",
    "            n_z = Latent space dimension\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # vae / ae definitions\n",
    "        self.n_z = args.n_z * 2 if vae_mode else args.n_z\n",
    "        self.forward = self.forward_vae if vae_mode else self.forward_ae\n",
    "\n",
    "        # Convolutional network\n",
    "        self.conv_part = DownUpConv(\n",
    "            args,\n",
    "            n_fea_next=args.n_fea_down,\n",
    "            move=\"down\",\n",
    "            pic_size=args.pic_size,\n",
    "            depth=args.crop_size,\n",
    "            n_fea_in=len(args.perspectives),\n",
    "        )\n",
    "        self.max_fea = self.conv_part.max_fea\n",
    "        self.hidden_dim = self.max_fea * args.min_size ** (args.dim)\n",
    "\n",
    "        # Finish with fully connected layers\n",
    "        self.fc_part = nn.Sequential(\n",
    "            # State size batch x (cur_fea*4*4*4)\n",
    "            nn.Linear(self.hidden_dim, self.n_z, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output size batch x n_z\n",
    "        )\n",
    "        # Initialise (conv part is already)\n",
    "        self.fc_part.apply(init_w)\n",
    "\n",
    "    def forward_vae(self, x):\n",
    "        \"\"\"calculate output, return mu and sigma\"\"\"\n",
    "        # Apply convolutions\n",
    "        x = self.conv_part(x)\n",
    "        # Resize\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        # Apply fully connected part\n",
    "        x = self.fc_part(x)\n",
    "        # Separate mu and sigma\n",
    "        mu, logvar = x.chunk(2, dim=1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def forward_ae(self, x):\n",
    "        \"\"\"calculate output, return mu and sigma\"\"\"\n",
    "        # Apply convolutions\n",
    "        x = self.conv_part(x)\n",
    "        # Resize\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        # Apply fully connected part\n",
    "        x = self.fc_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder class (also a Generator)\"\"\"\n",
    "\n",
    "    def __init__(self, args, init_w=weights_init):\n",
    "        \"\"\"\n",
    "        Setup the Architecture:\n",
    "        Args:\n",
    "            init_w = Function for initialisation of weights\n",
    "            n_chan = Number of input channels: batch x n_chan x depth x size x size\n",
    "            n_d_fea = Number of feature channels within network\n",
    "            n_z = Latent space dimension\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Convolutional network\n",
    "        self.conv_part = DownUpConv(\n",
    "            args,\n",
    "            n_fea_next=args.n_fea_up,\n",
    "            move=\"up\",\n",
    "            pic_size=args.pic_size,\n",
    "            depth=args.crop_size,\n",
    "            n_fea_in=len(args.perspectives),\n",
    "        )\n",
    "        self.max_fea = self.conv_part.max_fea\n",
    "        self.hidden_dim = self.max_fea * args.min_size ** (args.dim)\n",
    "        self.view_arr = [-1, self.max_fea]\n",
    "        self.view_arr.extend([args.min_size for _ in range(args.dim)])\n",
    "\n",
    "        self.fc_part = nn.Sequential(\n",
    "            # Input is batch x n_z\n",
    "            nn.Linear(args.n_z, self.hidden_dim, bias=False),\n",
    "            # nn.Tanh(),\n",
    "        )\n",
    "        # Initialise (conv part is already)\n",
    "        self.fc_part.apply(init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"calculate output\"\"\"\n",
    "        # Apply fully connected part\n",
    "        x = self.fc_part(x)\n",
    "        # Resize\n",
    "        x = x.view(self.view_arr)\n",
    "        # Apply convolutions\n",
    "        x = self.conv_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator class, only for true/fake differences\n",
    "    Classifier for Determinig between several classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, diag_dim=1, init_w=weights_init, wgan=False):\n",
    "        \"\"\"Setup the Architecture:\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Convolutional network\n",
    "        self.conv_part = DownUpConv(\n",
    "            args,\n",
    "            n_fea_next=args.n_fea_down,\n",
    "            move=\"down\",\n",
    "            pic_size=args.pic_size,\n",
    "            depth=args.crop_size,\n",
    "            n_fea_in=len(args.perspectives),\n",
    "            p_drop=args.p_drop,\n",
    "        )\n",
    "        self.max_fea = self.conv_part.max_fea\n",
    "        self.hidden_dim = self.max_fea * args.min_size ** (args.dim)\n",
    "\n",
    "        # Finish with fully connected layers\n",
    "        self.fc_part = nn.Sequential(\n",
    "            # State size batch x (cur_fea*4*4*4)\n",
    "            nn.Linear(self.hidden_dim, diag_dim, bias=False),\n",
    "        )\n",
    "        # Initialise (conv part is already)\n",
    "        self.fc_part.apply(init_w)\n",
    "        self.forward = self.forward_wgan if wgan else self.forward_dcgan\n",
    "\n",
    "    def forward_wgan(self, x):\n",
    "        \"\"\"calculate output, return prob real / fake\"\"\"\n",
    "\n",
    "        # Apply convolutions\n",
    "        x = self.conv_part(x)\n",
    "        # Resize\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        # Apply fully connected part\n",
    "        x = self.fc_part(x)\n",
    "        return x\n",
    "\n",
    "    def forward_dcgan(self, x):\n",
    "        \"\"\"calculate output, return prob real / fake\"\"\"\n",
    "\n",
    "        # Apply Network\n",
    "        x = self.forward_wgan(x)\n",
    "        torch.sigmoid_(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataloader.ipynb.\n",
      "Converted 01_architecture.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_parameters.ipynb.\n",
      "Converted 04_train_loop.ipynb.\n",
      "Converted 05_abstract_model.ipynb.\n",
      "Converted 10_diagnosis.ipynb.\n",
      "Converted 20_dcgan.ipynb.\n",
      "Converted 21_introvae.ipynb.\n",
      "Converted 22_vqvae.ipynb.\n",
      "Converted 23_bigan.ipynb.\n",
      "Converted 24_mocoae.ipynb.\n",
      "Converted 33_rnn_vae.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
