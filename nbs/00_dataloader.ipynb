{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "> Define the Dataset and Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os  # file and path management\n",
    "import torch  # machine learning\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader  # Structure for Dataloader\n",
    "from torchvision import transforms  # Predefined transformations\n",
    "from skimage import transform  # Rescale 2d images\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Dataset specific parameters\n",
    "MAX_PIXEL_VAL = 255\n",
    "MEAN = 58.09\n",
    "STDDEV = 49.73\n",
    "\n",
    "# only takes the middle range...\n",
    "INPUT_DIM = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The MRNet Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class MRNetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Magnetic Resonance Imaging Dataset of 1129 knee joints in training data and 119 for validation.\n",
    "    The Dataset contains information about:\n",
    "        root_dir: General file directory\n",
    "        contains two dictionarys with the corresponding infos:\n",
    "        dirs[{\"train\", \"valid\"}][{\"abn\", \"acl\", \"men\"}] -> abnormal, acl-rupture, meniscus\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, transform=None, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir(string): Path of the Dataset\n",
    "            transform: Transformations applied to a sample\n",
    "            mode: \"train\" or \"valid\"\n",
    "        \"\"\"\n",
    "        # lets assume the filepackage is correctly formatted: see order below\n",
    "        super(MRNetDataset, self).__init__()\n",
    "        self.root_dir = args.root_dir\n",
    "        self.naming = args.perspectives\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "        self.subpaths = os.listdir(self.root_dir)\n",
    "        assert (\n",
    "            len(self.subpaths) > 8\n",
    "        ), \"Not enough files in directory! - Check directory\"\n",
    "\n",
    "        self.dirs = {}\n",
    "        self.traindir = os.path.join(self.root_dir, \"train\")\n",
    "        self.dirs[\"train\"] = self.read_dir(self.traindir)\n",
    "\n",
    "        self.validdir = os.path.join(self.root_dir, \"valid\")\n",
    "        self.dirs[\"valid\"] = self.read_dir(self.validdir)\n",
    "\n",
    "        self.labels = {}\n",
    "        self.weights = {}\n",
    "        self.labels[\"train\"] = {}\n",
    "\n",
    "        # start with abnormal labels\n",
    "        labels = self.read_labels(os.path.join(self.root_dir, \"train-abnormal.csv\"))\n",
    "        self.labels[\"train\"][\"abn\"] = labels\n",
    "        neg_weight = np.mean(self.labels[\"train\"][\"abn\"])\n",
    "        self.weights[\"abn\"] = [neg_weight, 1 - neg_weight]\n",
    "\n",
    "        # acl labels\n",
    "        labels = self.read_labels(os.path.join(self.root_dir, \"train-acl.csv\"))\n",
    "        self.labels[\"train\"][\"acl\"] = labels\n",
    "        temp_labels = [\n",
    "            labels[i] for i in range(len(labels)) if self.labels[\"train\"][\"abn\"][i] == 1\n",
    "        ]\n",
    "        neg_weight = np.mean(temp_labels)\n",
    "        self.weights[\"acl\"] = [neg_weight, 1 - neg_weight]\n",
    "\n",
    "        # men labels and weights\n",
    "        labels = self.read_labels(os.path.join(self.root_dir, \"train-meniscus.csv\"))\n",
    "        self.labels[\"train\"][\"men\"] = labels\n",
    "        temp_labels = [\n",
    "            labels[i] for i in range(len(labels)) if self.labels[\"train\"][\"abn\"][i] == 1\n",
    "        ]\n",
    "        neg_weight = np.mean(temp_labels)\n",
    "        self.weights[\"men\"] = [neg_weight, 1 - neg_weight]\n",
    "\n",
    "        # validation labels without weights\n",
    "        self.labels[\"valid\"] = {}\n",
    "        self.labels[\"valid\"][\"abn\"] = self.read_labels(\n",
    "            os.path.join(self.root_dir, \"valid-abnormal.csv\")\n",
    "        )\n",
    "        self.labels[\"valid\"][\"acl\"] = self.read_labels(\n",
    "            os.path.join(self.root_dir, \"valid-acl.csv\")\n",
    "        )\n",
    "        self.labels[\"valid\"][\"men\"] = self.read_labels(\n",
    "            os.path.join(self.root_dir, \"valid-meniscus.csv\")\n",
    "        )\n",
    "\n",
    "        # length of the dataset\n",
    "        self.len = len(self.labels[self.mode][\"acl\"])\n",
    "\n",
    "        # define how the output should be formated\n",
    "        if args.model_type == \"diagnosis\":\n",
    "            self.get_img = self.image_stack\n",
    "        else:\n",
    "            self.get_img = self.volume_stack\n",
    "\n",
    "    def image_stack(self, mr_data):\n",
    "        \"\"\"\n",
    "        keep the pictures independent\n",
    "        \"\"\"\n",
    "        return mr_data\n",
    "\n",
    "    def volume_stack(self, mr_data):\n",
    "        \"\"\"\n",
    "        concatenate the pictures together\n",
    "        \"\"\"\n",
    "        return torch.cat([mr_data[name] for name in self.naming], dim=0)\n",
    "\n",
    "    def read_labels(self, datadir):\n",
    "        \"\"\"\n",
    "        Read the csv files and store as a single tensor\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        for i, line in enumerate(open(datadir).readlines()):\n",
    "            line = line.strip().split(\",\")\n",
    "            label = int(line[1])\n",
    "            labels.append(label)\n",
    "        return labels\n",
    "\n",
    "    def read_dir(self, dir_files):\n",
    "        \"\"\"\n",
    "        Read the directory and determine all files within\n",
    "        typically include all files from naming\n",
    "        Args:\n",
    "            dir_files: filepath for the folders with the categories\n",
    "        Return:\n",
    "            filenames: dict of filenames with their corresponding category(axial...)\n",
    "        \"\"\"\n",
    "        filenames = {}\n",
    "        # go trough ac, cor, sag\n",
    "        for subfolder in os.listdir(dir_files):\n",
    "            if subfolder in self.naming:\n",
    "                # get the local folder name\n",
    "                subname = os.path.join(dir_files, subfolder)\n",
    "                subfiles = []\n",
    "\n",
    "                # collect all filenames\n",
    "                for subfile in sorted(os.listdir(subname)):\n",
    "                    # exclude the .DS_STore file\n",
    "                    if subfile != \".DS_Store\":\n",
    "                        subfiles.append(os.path.join(subname, subfile))\n",
    "\n",
    "                filenames[subfolder] = subfiles\n",
    "\n",
    "        return filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of Knees\"\"\"\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx, apply_transform=True):\n",
    "        \"\"\"\n",
    "        Select an MRI dataset with the corresponding information by index.\n",
    "        Preprocess the data accordingly.\n",
    "        return: \n",
    "            data[\"vol\"] -> volumentric data\n",
    "            data[{\"abn\",\"acl\",\"men\"}] -> labels\n",
    "        \"\"\"\n",
    "        # load the corresponding files (input data)\n",
    "        mr_data = {}\n",
    "        for name in self.naming:\n",
    "            mr_data[name] = np.load(self.dirs[self.mode][name][idx])\n",
    "\n",
    "        # Apply transformations on the files here!\n",
    "        if self.transform and apply_transform:\n",
    "            for name in self.naming:\n",
    "                mr_data[name] = self.transform(mr_data[name])\n",
    "\n",
    "        # collect the csv data for the index (output data)\n",
    "        data = {}\n",
    "        data[\"img\"] = self.get_img(mr_data)\n",
    "        data[\"abn\"] = self.labels[self.mode][\"abn\"][idx]\n",
    "        data[\"acl\"] = self.labels[self.mode][\"acl\"][idx]\n",
    "        data[\"men\"] = self.labels[self.mode][\"men\"][idx]\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Randomly crop to only some slices of the image\"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"define the number of slices which will be cropped\"\"\"\n",
    "        assert isinstance(args.crop_size, int)\n",
    "        assert args.crop_size < 17  # watch out if sizes get too large\n",
    "        self.crop_size = args.crop_size\n",
    "\n",
    "    def __call__(self, mr_scan):\n",
    "        \"\"\"apply the cropping\"\"\"\n",
    "        # get the depth of the mr scan\n",
    "        depth = mr_scan.shape[0]\n",
    "        # choose random range within the depth\n",
    "        cr_start = np.random.randint(depth - self.crop_size)\n",
    "        # crop by returning only a part of the true scan\n",
    "        return mr_scan[cr_start : cr_start + self.crop_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TriplePrep(object):\n",
    "    \"\"\"\n",
    "    Randomly crop to only some slices of the image\n",
    "    Further pad the picture to the 224 size!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"define the number of slices which will be cropped\"\"\"\n",
    "        self.crop_percent = args.crop_percent\n",
    "        self.pic_size = 224\n",
    "        self.pad = int((args.pic_size - self.pic_size) / 2)\n",
    "        self.p_h = 0.5\n",
    "\n",
    "    def h_flip(self, mr_scan, depth):\n",
    "        \"\"\"apply horizontal flipping\"\"\"\n",
    "        # random check\n",
    "        if random.random() < self.p_h:\n",
    "            # flip\n",
    "            mr_scan = torch.flip(mr_scan, (2,))\n",
    "        return mr_scan\n",
    "\n",
    "    def __call__(self, mr_scan):\n",
    "        \"\"\"apply the cropping\"\"\"\n",
    "        mr_scan = torch.FloatTensor(mr_scan)\n",
    "        # get the depth of the mr scan\n",
    "        depth = mr_scan.shape[0]\n",
    "        # get the current crop-percentage:\n",
    "        cr_layers = int(depth * self.crop_percent)\n",
    "        # choose random range within the depth\n",
    "        cr_start = np.random.randint(depth - cr_layers)\n",
    "        # choose random range within width:\n",
    "        pad_w_b = np.random.randint(2 * self.pad)\n",
    "        pad_w_e = 2 * self.pad - pad_w_b\n",
    "        # choose random range within height:\n",
    "        pad_h_b = np.random.randint(2 * self.pad)\n",
    "        pad_h_e = 2 * self.pad - pad_h_b\n",
    "        # crop by returning only a part of the true scan and pad picture\n",
    "        mr_scan = mr_scan[\n",
    "            cr_start : cr_start + cr_layers, pad_w_b:-pad_w_e, pad_h_b:-pad_h_e\n",
    "        ]\n",
    "        # randonly flip the picture\n",
    "        mr_scan = self.h_flip(mr_scan, depth)\n",
    "        return mr_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class MiddleCrop(object):\n",
    "    \"\"\"Crop to only some slices of the image from the middle\"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"define the number of slices which will be cropped\"\"\"\n",
    "        assert isinstance(args.crop_size, int)\n",
    "        assert args.crop_size < 17  # watch out if sizes get too large\n",
    "        self.crop_size = args.crop_size\n",
    "\n",
    "    def __call__(self, mr_scan):\n",
    "        \"\"\"apply the cropping\"\"\"\n",
    "        # get the depth of the mr scan\n",
    "        depth = mr_scan.shape[0]\n",
    "        # choose the middle range within the scan\n",
    "        cr = int(depth / 2) - int(self.crop_size / 2)\n",
    "        # crop by returning only a part of the true scan\n",
    "        return mr_scan[cr : cr + self.crop_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale a whole MR set to a new size\"\"\"\n",
    "\n",
    "    def __init__(self, args, dtype=np.float32):\n",
    "        self.output_size = args.pic_size\n",
    "        self.dim = args.dim\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.shorten = False\n",
    "        self.model = args.model_type\n",
    "\n",
    "        if self.model == \"diagnosis\":\n",
    "            self.shorten = True\n",
    "\n",
    "    def __call__(self, mr_scan):\n",
    "        \"\"\"Apply rescaling\"\"\"\n",
    "\n",
    "        if self.shorten:\n",
    "            return mr_scan\n",
    "\n",
    "        # get old dimensions\n",
    "        depth, h, w = mr_scan.shape\n",
    "\n",
    "        # get new dimensions\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        # if the resolution is the same, avoid this in the future\n",
    "        if (new_h, new_w) != (h, w):\n",
    "            # this resets this to a numpy array\n",
    "            mr_scan = transform.resize(mr_scan, (depth, new_h, new_w))\n",
    "\n",
    "        # rescale to required input dimension\n",
    "        if self.dim == 3 and self.shorten == False:\n",
    "            mr_scan = mr_scan.reshape((1, depth, new_h, new_w))\n",
    "\n",
    "        return mr_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, type=torch.FloatTensor):\n",
    "        self.type = type\n",
    "\n",
    "    def __call__(self, mr_scan):\n",
    "        # apply transformation to Float32 tensor\n",
    "        return self.type(mr_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize the array\"\"\"\n",
    "\n",
    "    def __init__(self, usegpu=True):\n",
    "        \"\"\"save mean and std\"\"\"\n",
    "\n",
    "        if usegpu:\n",
    "            self.min = torch.min\n",
    "            self.max = torch.max\n",
    "        else:\n",
    "            self.min = np.min\n",
    "            self.max = np.max\n",
    "\n",
    "    def __call__(self, mr_scan):\n",
    "        \"\"\"apply normalization on scan\"\"\"\n",
    "        # preselect min and max\n",
    "        min_mr = self.min(mr_scan)\n",
    "        max_mr = self.max(mr_scan)\n",
    "\n",
    "        # standardize\n",
    "        mr_scan = (mr_scan - min_mr) / (max_mr - min_mr) * MAX_PIXEL_VAL\n",
    "        # normalize\n",
    "        mr_scan = (mr_scan - MEAN) / STDDEV\n",
    "\n",
    "        return mr_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def load_datasets(args):\n",
    "    \"\"\"Retruns the training and validation dataset with the corresponding loader\"\"\"\n",
    "    # Select the right cropping\n",
    "    Crop = RandomCrop if args.rand_crop == True else MiddleCrop\n",
    "\n",
    "    # special case with triplenet -> we can have more layers -> take percentage\n",
    "    if args.model_type == \"diagnosis\":\n",
    "        args.batch_size = 1\n",
    "        Crop = TriplePrep\n",
    "\n",
    "    # Transformations for training\n",
    "    transform = transforms.Compose(\n",
    "        [Normalize(usegpu=False), Crop(args), Rescale(args), ToTensor(),]\n",
    "    )\n",
    "\n",
    "    # Create Datasets\n",
    "    train_dataset = MRNetDataset(args, mode=\"train\", transform=transform)\n",
    "\n",
    "    store = args.crop_percent\n",
    "    args.crop_percent = 0.99\n",
    "\n",
    "    # Transformations for validation\n",
    "    transform = transforms.Compose(\n",
    "        [Normalize(usegpu=False), Crop(args), Rescale(args), ToTensor(),]\n",
    "    )\n",
    "\n",
    "    args.crop_percent = store\n",
    "\n",
    "    valid_dataset = MRNetDataset(args, mode=\"valid\", transform=transform)\n",
    "\n",
    "    # Create the Dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_worker,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_worker,\n",
    "    )\n",
    "\n",
    "    return train_dataset, valid_dataset, train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def test_dataset(args, valid_loader):\n",
    "    \"\"\"\n",
    "    return the required model depending on the arguments:\n",
    "    \"\"\"\n",
    "    test_data = next(iter(valid_loader))\n",
    "    print(\"Input dimension:\")\n",
    "\n",
    "    if args.model_type == \"diagnosis\":\n",
    "        print(test_data[\"img\"][args.perspectives[0]].shape)\n",
    "        print(torch.min(test_data[\"img\"][args.perspectives[0]]))\n",
    "        print(torch.max(test_data[\"img\"][args.perspectives[0]]))\n",
    "    else:\n",
    "        print(test_data[\"img\"].shape)\n",
    "        print(torch.min(test_data[\"img\"]))\n",
    "        print(torch.max(test_data[\"img\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def load_test_batch(args):\n",
    "    \"\"\"\n",
    "    This function creates a \"pseudo\" batch for simple testing\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    data[\"img\"] = {}\n",
    "\n",
    "    # structure separately\n",
    "    if args.model_type == \"diagnosis\":\n",
    "\n",
    "        for name in args.perspectives:\n",
    "            slices = np.random.randint(1, 60)\n",
    "            data[\"img\"][name] = torch.randn(1, slices, 224, 224)\n",
    "\n",
    "        for cla in args.classes:\n",
    "            target = np.random.randint(0, 2)\n",
    "            data[cla] = target\n",
    "\n",
    "    # structure coupled\n",
    "    else:\n",
    "        if args.dim == 3:\n",
    "            data[\"img\"] = torch.randn(\n",
    "                args.batch_size,\n",
    "                len(args.perspectives),\n",
    "                args.crop_size,\n",
    "                args.pic_size,\n",
    "                args.pic_size,\n",
    "            )\n",
    "        else:\n",
    "            data[\"img\"] = torch.randn(\n",
    "                args.batch_size, len(args.perspectives), args.pic_size, args.pic_size\n",
    "            )\n",
    "        for cla in args.classes:\n",
    "            target = np.random.randint(0, 2, args.batch_size)\n",
    "            data[cla] = target\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from deeptool.parameters import get_all_args, compat_args\n",
    "\n",
    "args = get_all_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 16, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch for VQVAE 3D\n",
    "args.model_type = \"vqvae\"\n",
    "args.dim = 3\n",
    "args = compat_args(args)\n",
    "batch = load_test_batch(args)\n",
    "batch[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch for VQVAE 2D\n",
    "args.model_type = \"vqvae\"\n",
    "args.dim = 2\n",
    "args = compat_args(args)\n",
    "batch = load_test_batch(args)\n",
    "batch[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 224, 224])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch for Diagnosis -> only 3D\n",
    "args.model_type = \"diagnosis\"\n",
    "args.dim = 3\n",
    "args = compat_args(args)\n",
    "batch = load_test_batch(args)\n",
    "batch[\"img\"][\"axial\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataloader.ipynb.\n",
      "Converted 01_architecture.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_parameters.ipynb.\n",
      "Converted 04_train_loop.ipynb.\n",
      "Converted 10_diagnosis.ipynb.\n",
      "Converted 20_dcgan.ipynb.\n",
      "Converted 21_introvae.ipynb.\n",
      "Converted 22_vqvae.ipynb.\n",
      "Converted 23_bigan.ipynb.\n",
      "Converted 33_rnn_vae.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
