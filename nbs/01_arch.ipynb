{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Init function and single Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Define the weight parameters depending on the type:\n",
    "    Conv or Batchnorm\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BlurPool3d(nn.Module):\n",
    "    \"\"\"\n",
    "    Modification of: https://arxiv.org/abs/1904.11486 for 3d purpose.\n",
    "    Copyright (c) 2019, Adobe Inc. All rights reserved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filt_size=3, stride=2, channels=None, pad_off=0):\n",
    "\n",
    "        super(BlurPool3d, self).__init__()\n",
    "        self.filt_size = filt_size\n",
    "        self.pad_off = pad_off\n",
    "        self.pad_sizes = [int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2)),\n",
    "                          int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2)),\n",
    "                          int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2))]\n",
    "        self.pad_sizes = [pad_size+pad_off for pad_size in self.pad_sizes]\n",
    "        self.stride = stride\n",
    "        self.channels = channels\n",
    "\n",
    "        if(self.filt_size == 1):\n",
    "            a = np.array([1., ])\n",
    "        elif(self.filt_size == 2):\n",
    "            a = np.array([1., 1.])\n",
    "        elif(self.filt_size == 3):\n",
    "            a = np.array([1., 2., 1.])\n",
    "        elif(self.filt_size == 4):\n",
    "            a = np.array([1., 3., 3., 1.])\n",
    "        elif(self.filt_size == 5):\n",
    "            a = np.array([1., 4., 6., 4., 1.])\n",
    "        elif(self.filt_size == 6):\n",
    "            a = np.array([1., 5., 10., 10., 5., 1.])\n",
    "        elif(self.filt_size == 7):\n",
    "            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n",
    "\n",
    "        filt = torch.Tensor(\n",
    "            a[:, None, None] * a[None, None, :] * a[None, :,  None])\n",
    "        filt = filt/torch.sum(filt)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'filt', filt[None, None, :, :, :].repeat((self.channels, 1,  1, 1, 1)))\n",
    "        # Only pad possible for 3d use\n",
    "        self.pad = nn.ReplicationPad3d(self.pad_sizes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv3d(self.pad(x), self.filt, stride=self.stride, groups=x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Quantize(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantization 'Layer'\n",
    "    inspired by: https://github.com/deepmind/sonnet\n",
    "    modified from: https://github.com/rosinality/vq-vae-2-pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Setup the embedding Matrix (dim x n_embed)\n",
    "        \"\"\"\n",
    "        # This is equal to the feature number at the corresponding layer\n",
    "        self.dim = dim\n",
    "        # This is the discretization level for each pixel\n",
    "        self.n_embed = n_embed\n",
    "        # Learning parameters\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        # Init matrix of available embeddings\n",
    "        embed = torch.randn(dim, n_embed)\n",
    "        # Register to avoid gradients\n",
    "        self.register_buffer('embed', embed)\n",
    "        self.register_buffer('cluster_size', torch.zeros(n_embed))\n",
    "        self.register_buffer('embed_avg', embed.clone())\n",
    "\n",
    "    def forward(self, x, update=True):\n",
    "        \"\"\"\n",
    "        Apply the Quantization on the input:\n",
    "        -> Update Embeddings\n",
    "        -> Remain the Gradient flow\n",
    "        Return:\n",
    "            Quantized input\n",
    "            Difference between true and\n",
    "        \"\"\"\n",
    "        # Reshape input\n",
    "        flatten = x.reshape(-1, self.dim)\n",
    "        # Calculate L2-distance between each pixel / voxel and embedding\n",
    "        dist = (\n",
    "            flatten.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * flatten @ self.embed\n",
    "            + self.embed.pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "        # Select the closest pairs\n",
    "        _, embed_ind = (-dist).max(1)\n",
    "        # Construct matrix of matching pairs\n",
    "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
    "        # Select the correct dimensions here!\n",
    "        embed_ind = embed_ind.view(*x.shape[:-1])\n",
    "        # Apply quantization\n",
    "        quantize = self.embed_code(embed_ind)\n",
    "\n",
    "        # Only for training -> Update Embeddings with moving average\n",
    "        if update:\n",
    "            # N_i = N_(i-1) * gamma + (1-gamma) * n_i\n",
    "            self.cluster_size.data.mul_(self.decay).add_(\n",
    "                1 - self.decay, embed_onehot.sum(0)\n",
    "            )\n",
    "            # Sum(E(x))\n",
    "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
    "            # m_i = m_(i-1) *gamma + (1-gamma) * Sum(E(x))\n",
    "            self.embed_avg.data.mul_(self.decay).add_(\n",
    "                1 - self.decay, embed_sum)\n",
    "            # N_i\n",
    "            n = self.cluster_size.sum()\n",
    "            # norm N_i\n",
    "            cluster_size = (\n",
    "                (self.cluster_size + self.eps) /\n",
    "                (n + self.n_embed * self.eps) * n\n",
    "            )\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
    "            # e_i = m_i / N_i\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        # Loss between Original to Quantization\n",
    "        diff = (quantize.detach() - x).pow(2).mean()\n",
    "        # Get Output, while enabling to copy gradients\n",
    "        quantize = x + (quantize - x).detach()\n",
    "        # Return results\n",
    "        return quantize, diff, embed_ind\n",
    "\n",
    "    def embed_code(self, embed_id):\n",
    "        \"\"\"\n",
    "        Perform the quantization by selecting all embedings from the ids\n",
    "        \"\"\"\n",
    "        return F.embedding(embed_id, self.embed.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16, 8, 64]) tensor(1.3022) torch.Size([16, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "quant = Quantize(64, 100)\n",
    "input = torch.randn(16, 16, 8, 64)\n",
    "quantize, diff, embed_ind = quant(input)\n",
    "print(quantize.shape, diff, embed_ind.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Block of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An individually designalble ResNet Block for 3 Dimensional Convoluions\n",
    "    based on: https://arxiv.org/abs/1512.03385\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_chan, convsize=3,\n",
    "                 activation=nn.ReLU(inplace=True),\n",
    "                 init_w=weights_init, dim=3):\n",
    "        \"\"\"setup the general architecture\"\"\"\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        if dim == 3:\n",
    "            Conv = nn.Conv3d\n",
    "            BatchNorm = nn.BatchNorm3d\n",
    "        else:\n",
    "            Conv = nn.Conv2d\n",
    "            BatchNorm = nn.BatchNorm2d\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.main_part = nn.Sequential(\n",
    "            # Layer 1\n",
    "            Conv(n_chan, n_chan, convsize,\n",
    "                 stride=1, padding=1, bias=False),\n",
    "            BatchNorm(n_chan),\n",
    "            self.activation,\n",
    "            # Layer 2\n",
    "            Conv(n_chan, n_chan, convsize,\n",
    "                 stride=1, padding=1, bias=False),\n",
    "            BatchNorm(n_chan),\n",
    "        )\n",
    "        self.main_part.apply(init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the forward pass\"\"\"\n",
    "        residual = x\n",
    "        out = self.main_part(x)\n",
    "        out += residual\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ConvBn(nn.Module):\n",
    "    \"\"\"\n",
    "    An individually designalble Block for 3 Dimensional Convoluions with Batchnorm and Dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chan, out_chan, convsize=3,\n",
    "                 stride=2, activation=nn.LeakyReLU(0.2, inplace=True),\n",
    "                 init_w=weights_init, padding=1, dim=3, p_drop=0):\n",
    "        \"\"\"setup the general architecture\"\"\"\n",
    "        super(ConvBn, self).__init__()\n",
    "\n",
    "        if dim == 3:\n",
    "            Conv = nn.Conv3d\n",
    "            BatchNorm = nn.BatchNorm3d\n",
    "            Dropout = nn.Dropout3d\n",
    "        else:\n",
    "            Conv = nn.Conv2d\n",
    "            BatchNorm = nn.BatchNorm2d\n",
    "            Dropout = nn.Dropout2d\n",
    "            # Check convsize and stride\n",
    "            if type(convsize) == tuple:\n",
    "                if len(convsize) > 2:\n",
    "                    convsize = convsize[1:]\n",
    "            if type(stride) == tuple:\n",
    "                if len(stride) > 2:\n",
    "                    stride = stride[1:]\n",
    "\n",
    "        self.main_part = nn.Sequential(\n",
    "            Conv(in_chan, out_chan, convsize,\n",
    "                 stride=stride, padding=padding, bias=False),\n",
    "            BatchNorm(out_chan),\n",
    "            activation,\n",
    "            Dropout(p=p_drop),\n",
    "        )\n",
    "        self.main_part.apply(init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the forward pass\"\"\"\n",
    "        return self.main_part(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Test it!\n",
    "m = ConvBn(1, 16, convsize=4, stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "inp = torch.randn(20, 1, 16, 64, 64)\n",
    "output = m(inp)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ConvTpBn(nn.Module):\n",
    "    \"\"\"\n",
    "    An individually designalble Block for 3 Dimensional Transposed Convoluions with Batchnorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chan, out_chan, convsize=3,\n",
    "                 stride=2, activation=nn.ReLU(inplace=True),\n",
    "                 init_w=weights_init, padding=1, dim=3):\n",
    "        \"\"\"setup the general architecture\"\"\"\n",
    "        super(ConvTpBn, self).__init__()\n",
    "        if dim == 3:\n",
    "            ConvTranspose = nn.ConvTranspose3d\n",
    "            BatchNorm = nn.BatchNorm3d\n",
    "        else:\n",
    "            ConvTranspose = nn.ConvTranspose2d\n",
    "            BatchNorm = nn.BatchNorm2d\n",
    "            # Check convsize and stride\n",
    "            if type(convsize) == tuple:\n",
    "                if len(convsize) > 2:\n",
    "                    convsize = convsize[1:]\n",
    "            if type(stride) == tuple:\n",
    "                if len(stride) > 2:\n",
    "                    stride = stride[1:]\n",
    "\n",
    "        self.main_part = nn.Sequential(\n",
    "            ConvTranspose(in_chan, out_chan, convsize,\n",
    "                          stride=stride, padding=padding, bias=False),\n",
    "            BatchNorm(out_chan),\n",
    "            activation,\n",
    "        )\n",
    "        self.main_part.apply(init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the forward pass\"\"\"\n",
    "        return self.main_part(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 31, 127, 127])\n"
     ]
    }
   ],
   "source": [
    "m = ConvTpBn(1, 16, convsize=3, stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "output = m(inp)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LinearSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper class to provide a simple ending with linear layer and Sigmoid\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, y_dim, bias=False):\n",
    "        \"\"\"setup network\"\"\"\n",
    "        super(LinearSigmoid, self).__init__()\n",
    "        # store inputs\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.bias = bias\n",
    "        # make network\n",
    "        self.main_part = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.y_dim, bias=self.bias),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"reformat then apply fc part\"\"\"\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        x = self.main_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 The generic convolutional network block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DownUpConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Type, which contains the generic conv network\n",
    "    for 3d up- or downsclaing depending on \"move\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, n_fea_in, n_fea_next, pic_size, depth, move=\"down\", p_drop=0):\n",
    "        \"\"\"Setup the conv-network\"\"\"\n",
    "        super(DownUpConv, self).__init__()\n",
    "        # Convolutions in 2d / 3d\n",
    "        self.dim = args.dim\n",
    "        # Input dim\n",
    "        self.pic_size = pic_size\n",
    "        self.depth = depth\n",
    "        self.n_fea_in = n_fea_in\n",
    "        # Number of feature channels\n",
    "        self.n_fea_next = n_fea_next\n",
    "        # Output dim\n",
    "        self.min_size = args.min_size  # when to stop reducing\n",
    "        # Scaling\n",
    "        self.scale2d = args.scale2d\n",
    "        self.scale3d = args.scale3d\n",
    "        self.n_res2d = args.n_res2d\n",
    "        self.n_res3d = args.n_res3d\n",
    "        # Dropout\n",
    "        self.p_drop = p_drop\n",
    "        # Add the relevant quantization layers if required\n",
    "        self.vq_layers = args.vq_layers if args.model_type == \"vqvae\" else []\n",
    "        # Direction\n",
    "        self.move = move  # Define whether to scale up or down\n",
    "        self.main, output_tuple = self.generic_conv_init()\n",
    "        self.max_fea, self.max_fea_next, self.pic_out, self.final_depth = output_tuple\n",
    "\n",
    "    def add_layers(self, conv_layers, fea_in, fea_out, n_res, pic_size, convsize=4, stride=2):\n",
    "        \"\"\"\n",
    "        Add layers according to number of residual blocks and down/upscale mode\n",
    "        \"\"\"\n",
    "        # Downsample\n",
    "        # ---------------------------------------------------\n",
    "        if self.move == \"down\":\n",
    "            # 1. Downsample\n",
    "            conv_layers.extend([\n",
    "                ConvBn(fea_in, fea_out, dim=self.dim,\n",
    "                       convsize=convsize, stride=stride, p_drop=self.p_drop),\n",
    "            ])\n",
    "            # 2. Add the residual blocks:\n",
    "            for _ in range(n_res):\n",
    "                conv_layers.extend([\n",
    "                    ResNetBlock(fea_out, convsize=3, dim=self.dim),\n",
    "                ])\n",
    "\n",
    "        # Upsample\n",
    "        # ---------------------------------------------------\n",
    "        else:\n",
    "            # Attention! This layer can be a quantization layer!\n",
    "            special_pic = pic_size / 2\n",
    "            if special_pic > self.min_size and special_pic in self.vq_layers:\n",
    "                fea_out *= 2  # so it has a doubled feature size\n",
    "\n",
    "            # 1. Add the residual blocks:\n",
    "            for _ in range(n_res):\n",
    "                conv_layers[:0] = ([\n",
    "                    ResNetBlock(fea_in, convsize=3, dim=self.dim),\n",
    "                ])\n",
    "\n",
    "            # 2. Upsample\n",
    "            conv_layers[:0] = ([\n",
    "                ConvTpBn(fea_out, fea_in, dim=self.dim,\n",
    "                         convsize=convsize, stride=stride),\n",
    "            ])\n",
    "\n",
    "    def generic_conv_init(self):\n",
    "        \"\"\"\n",
    "        Initialise the convolution layers generically:\n",
    "        Down:\n",
    "        Idea: -> Half picsize until all 3 dims are equal\n",
    "              -> Then reduce all dims until min_size.\n",
    "        Up:\n",
    "        Idea: -> Double all dims until z-limit is reached\n",
    "              -> Then double picsize until output-dim is reached.\n",
    "        \"\"\"\n",
    "\n",
    "        # Init the conv_layer list\n",
    "        conv_layers = []\n",
    "        # Current z-dim of the picture\n",
    "        cur_pic_dim = self.pic_size\n",
    "        # Current anz of features at input size\n",
    "        cur_fea_in = self.n_fea_in\n",
    "        # Current anz of features at output size\n",
    "        cur_fea_out = self.n_fea_next\n",
    "        # Current depth of the picture\n",
    "        cur_depth = self.depth\n",
    "        # Summarize current values\n",
    "        output_tuple = (cur_fea_in, cur_fea_out, cur_pic_dim, cur_depth)\n",
    "\n",
    "        # Until the limiting z_dim occurs or picsize too small\n",
    "        # ---------------------------------------------------------------\n",
    "        while cur_pic_dim > self.depth and cur_pic_dim > self.min_size:\n",
    "            # Add layers\n",
    "            self.add_layers(\n",
    "                conv_layers, cur_fea_in, cur_fea_out, self.n_res2d,\n",
    "                cur_pic_dim, convsize=(3, 4, 4), stride=(1, 2, 2))\n",
    "            # Update input size\n",
    "            cur_fea_in = cur_fea_out\n",
    "            # Features are doupled\n",
    "            cur_fea_out *= self.scale2d\n",
    "            cur_pic_dim /= 2  # dimension is halved\n",
    "\n",
    "            # Store current values\n",
    "            output_tuple = (cur_fea_in, cur_fea_out, cur_pic_dim, cur_depth)\n",
    "\n",
    "            # CASE: Layer is a quantization layer!!\n",
    "            if cur_pic_dim in self.vq_layers:\n",
    "                # Return current Network and relevant parameters\n",
    "                return nn.Sequential(*conv_layers), output_tuple\n",
    "\n",
    "        # adjust for new scale factor\n",
    "        # cur_fea_out = max([cur_fea_in * self.scale3d, self.n_fea_next])\n",
    "\n",
    "        # Limit reached, now continue until min-picsize reached\n",
    "        # ---------------------------------------------------------------\n",
    "        while cur_pic_dim > self.min_size:\n",
    "            # Add layers\n",
    "            self.add_layers(\n",
    "                conv_layers, cur_fea_in, cur_fea_out, self.n_res3d,\n",
    "                cur_pic_dim, convsize=4, stride=2)\n",
    "            # Update input size\n",
    "            cur_fea_in = cur_fea_out\n",
    "            # Features are doupled\n",
    "            cur_fea_out *= self.scale3d\n",
    "            cur_pic_dim /= 2  # Dimension is halved\n",
    "            cur_depth /= 2  # Depth is also halfed\n",
    "\n",
    "            # Store current values\n",
    "            output_tuple = (cur_fea_in, cur_fea_out, cur_pic_dim, cur_depth)\n",
    "\n",
    "            # CASE: Layer is a quantization layer!!\n",
    "            if cur_pic_dim in self.vq_layers:\n",
    "                # Return current Network and relevant parameters\n",
    "                return nn.Sequential(*conv_layers), output_tuple\n",
    "\n",
    "        # Finally return the network at minimum size\n",
    "        return nn.Sequential(*conv_layers), output_tuple\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the forward pass\"\"\"\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Whole Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder with 3dimensional conv setup\"\"\"\n",
    "\n",
    "    def __init__(self, args, init_w=weights_init, vae_mode=True):\n",
    "        \"\"\"\n",
    "        Setup the Architecture:\n",
    "        Args:\n",
    "            ngpu = Number of GPUs available\n",
    "            init_w = Function for initialisation of weights\n",
    "            n_chan = Number of input channels: batch x n_chan x depth x size x size\n",
    "            n_d_fea = Number of feature channels within network\n",
    "            n_z = Latent space dimension\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # vae / ae definitions\n",
    "        self.n_z = args.n_z * 2 if vae_mode else args.n_z\n",
    "        self.forward = self.forward_vae if vae_mode else self.forward_ae\n",
    "\n",
    "        # Convolutional network\n",
    "        self.conv_part = DownUpConv(args, n_fea_next=args.n_fea_down, move=\"down\",\n",
    "                                    pic_size=args.pic_size, depth=args.crop_size,\n",
    "                                    n_fea_in=len(args.perspectives))\n",
    "        self.max_fea = self.conv_part.max_fea\n",
    "        self.hidden_dim = self.max_fea * args.min_size**(args.dim)\n",
    "\n",
    "        # Finish with fully connected layers\n",
    "        self.fc_part = nn.Sequential(\n",
    "            # State size batch x (cur_fea*4*4*4)\n",
    "            nn.Linear(self.hidden_dim, self.n_z, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output size batch x n_z\n",
    "        )\n",
    "        # Initialise (conv part is already)\n",
    "        self.fc_part.apply(init_w)\n",
    "\n",
    "    def forward_vae(self, x):\n",
    "        \"\"\"calculate output, return mu and sigma\"\"\"\n",
    "        # Apply convolutions\n",
    "        x = self.conv_part(x)\n",
    "        # Resize\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        # Apply fully connected part\n",
    "        x = self.fc_part(x)\n",
    "        # Separate mu and sigma\n",
    "        mu, logvar = x.chunk(2, dim=1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def forward_ae(self, x):\n",
    "        \"\"\"calculate output, return mu and sigma\"\"\"\n",
    "        # Apply convolutions\n",
    "        x = self.conv_part(x)\n",
    "        # Resize\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        # Apply fully connected part\n",
    "        x = self.fc_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder class (also a Generator)\"\"\"\n",
    "\n",
    "    def __init__(self, args, init_w=weights_init):\n",
    "        \"\"\"\n",
    "        Setup the Architecture:\n",
    "        Args:\n",
    "            init_w = Function for initialisation of weights\n",
    "            n_chan = Number of input channels: batch x n_chan x depth x size x size\n",
    "            n_d_fea = Number of feature channels within network\n",
    "            n_z = Latent space dimension\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Convolutional network\n",
    "        self.conv_part = DownUpConv(args, n_fea_next=args.n_fea_up, move=\"up\",\n",
    "                                    pic_size=args.pic_size, depth=args.crop_size,\n",
    "                                    n_fea_in=len(args.perspectives))\n",
    "        self.max_fea = self.conv_part.max_fea\n",
    "        self.hidden_dim = self.max_fea * args.min_size**(args.dim)\n",
    "        self.view_arr = [-1, self.max_fea]\n",
    "        self.view_arr.extend([args.min_size for _ in range(args.dim)])\n",
    "\n",
    "        self.fc_part = nn.Sequential(\n",
    "            # Input is batch x n_z\n",
    "            nn.Linear(args.n_z, self.hidden_dim, bias=False),\n",
    "            # nn.Tanh(),\n",
    "        )\n",
    "        # Initialise (conv part is already)\n",
    "        self.fc_part.apply(init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"calculate output\"\"\"\n",
    "        # Apply fully connected part\n",
    "        x = self.fc_part(x)\n",
    "        # Resize\n",
    "        x = x.view(self.view_arr)\n",
    "        # Apply convolutions\n",
    "        x = self.conv_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator class, only for true/fake differences\n",
    "    Classifier for Determinig between several classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, diag_dim=1, init_w=weights_init, wgan=False):\n",
    "        \"\"\"Setup the Architecture:\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Convolutional network\n",
    "        self.conv_part = DownUpConv(args, n_fea_next=args.n_fea_down, move=\"down\",\n",
    "                                    pic_size=args.pic_size, depth=args.crop_size,\n",
    "                                    n_fea_in=len(args.perspectives), p_drop=args.p_drop)\n",
    "        self.max_fea = self.conv_part.max_fea\n",
    "        self.hidden_dim = self.max_fea * args.min_size**(args.dim)\n",
    "\n",
    "        # Finish with fully connected layers\n",
    "        self.fc_part = nn.Sequential(\n",
    "            # State size batch x (cur_fea*4*4*4)\n",
    "            nn.Linear(self.hidden_dim, diag_dim, bias=False),\n",
    "        )\n",
    "        # Initialise (conv part is already)\n",
    "        self.fc_part.apply(init_w)\n",
    "        self.forward = self.forward_wgan if wgan else self.forward_dcgan\n",
    "\n",
    "    def forward_wgan(self, x):\n",
    "        \"\"\"calculate output, return prob real / fake\"\"\"\n",
    "\n",
    "        # Apply convolutions\n",
    "        x = self.conv_part(x)\n",
    "        # Resize\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        # Apply fully connected part\n",
    "        x = self.fc_part(x)\n",
    "        return x\n",
    "\n",
    "    def forward_dcgan(self, x):\n",
    "        \"\"\"calculate output, return prob real / fake\"\"\"\n",
    "\n",
    "        # Apply Network\n",
    "        x = self.forward_wgan(x)\n",
    "        torch.sigmoid_(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_demo.ipynb.\n",
      "Converted 01_architecture.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
