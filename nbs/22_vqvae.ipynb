{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.vqvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQVAE (2)\n",
    "\n",
    "> Application of the Vector Quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/vqvae.png\" width = 600/>\n",
    "     \n",
    "https://arxiv.org/pdf/1906.00446.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# General defines\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "\n",
    "# Personal imports\n",
    "from deeptool.architecture import DownUpConv, Quantize, LinearSigmoid, Discriminator\n",
    "from deeptool.abs_model import AbsModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Apply Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class EncQuantDec(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper Class for the generic generated Network with variable number of Quantization Layers \n",
    "    It Contains:\n",
    "        Enc   <- List of Encoders\n",
    "        Dec   <- List of Decoders\n",
    "        Quant <- List of Quantizations\n",
    "    If Required:\n",
    "        Cla   <- List of Classifiers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Idea: Create as many networks, until the minimum dimension is reached\n",
    "        Add them to a list and construct Quant and Decoders accordingly\n",
    "        \"\"\"\n",
    "        super(EncQuantDec, self).__init__()\n",
    "\n",
    "        # Initialise empty list of networks -> using Module List to track parameters\n",
    "        self.Enc = nn.ModuleList()\n",
    "        self.Dec = nn.ModuleList()\n",
    "        self.Quant = nn.ModuleList()\n",
    "\n",
    "        # The Classify / Discriminator part:\n",
    "        self.classify = False\n",
    "        if args.vq_classify:\n",
    "            self.Cla = nn.ModuleList()\n",
    "            self.classify = True\n",
    "            self.y_dim = len(args.classes)\n",
    "\n",
    "        # Gan simply decides between true / false\n",
    "        if args.vq_gan and False:  # deacivated for now!!\n",
    "            self.Cla = nn.ModuleList()\n",
    "            self.classify = True\n",
    "            self.y_dim = 1\n",
    "\n",
    "        # Current dimensions\n",
    "        n_chan = len(args.perspectives)\n",
    "        arr_pic_size = [args.pic_size]\n",
    "        arr_fea_in = [n_chan]\n",
    "        arr_fea_out = [args.n_fea_up]\n",
    "        arr_depth = [args.crop_size]\n",
    "\n",
    "        # Create encoder networks until the minimum size is reached\n",
    "        # Also Create the Classification layers\n",
    "        count = 0\n",
    "        while arr_pic_size[-1] > args.min_size:\n",
    "            # Create next Network\n",
    "            cur_net = DownUpConv(\n",
    "                args,\n",
    "                n_fea_in=arr_fea_in[-1],\n",
    "                n_fea_next=arr_fea_out[-1],\n",
    "                pic_size=arr_pic_size[-1],\n",
    "                depth=arr_depth[-1],\n",
    "                move=\"down\",\n",
    "            )\n",
    "\n",
    "            # Save the current dimensions\n",
    "            arr_fea_in.append(cur_net.max_fea)\n",
    "            arr_fea_out.append(cur_net.max_fea_next)\n",
    "            arr_depth.append(cur_net.final_depth)\n",
    "            arr_pic_size.append(cur_net.pic_out)\n",
    "            # Append this net to the list of Encoding networks\n",
    "            self.Enc.append(cur_net)\n",
    "\n",
    "            # Add the classification layers as well\n",
    "            if self.classify:\n",
    "                # determin the factor of features for classification\n",
    "                if count == 0:\n",
    "                    # first layer\n",
    "                    fea_fac = 1\n",
    "                    count += 1\n",
    "                else:\n",
    "                    # all middler layers\n",
    "                    fea_fac = 3\n",
    "\n",
    "                # create classification networks\n",
    "                cur_net = DownUpConv(\n",
    "                    args,\n",
    "                    n_fea_in=arr_fea_in[-2] * fea_fac,\n",
    "                    n_fea_next=arr_fea_out[-2],\n",
    "                    pic_size=arr_pic_size[-2],\n",
    "                    depth=arr_depth[-2],\n",
    "                    move=\"down\",\n",
    "                )\n",
    "                self.Cla.append(cur_net)\n",
    "\n",
    "                # add the final layer if min size is reached:\n",
    "                if arr_pic_size[-1] <= args.min_size:\n",
    "                    # output dim\n",
    "                    y_dim = self.y_dim\n",
    "                    # input dim (Class + Quant-single input)\n",
    "                    hidden_dim = 2 * arr_fea_in[-1] * args.min_size ** (args.dim)\n",
    "                    # add a simple Linear Net with Sigmoid activation\n",
    "                    cur_net = LinearSigmoid(hidden_dim, y_dim)\n",
    "                    # include at end of network\n",
    "                    self.Cla.append(cur_net)\n",
    "\n",
    "        # Add the Quantization layers starting from smallest dimension\n",
    "        for fea_in, pic_size in zip(\n",
    "            reversed(arr_fea_in[1:]), reversed(arr_pic_size[1:])\n",
    "        ):\n",
    "            # Double the feature input if we dont deal with the last layer\n",
    "            if pic_size != args.min_size:\n",
    "                fea_in *= 2\n",
    "            # The last layer differs:\n",
    "            quant = Quantize(fea_in, args.vq_n_embed, decay=args.vq_gamma)\n",
    "            self.Quant.append(quant)\n",
    "\n",
    "        # Reverse the lists for Decoder construction, last elements are not needed\n",
    "        arr_fea_in = reversed(arr_fea_in[:-1])\n",
    "        arr_fea_out = reversed(arr_fea_out[:-1])\n",
    "        arr_pic_size = reversed(arr_pic_size[:-1])\n",
    "        arr_depth = reversed(arr_depth[:-1])\n",
    "\n",
    "        # Now add the corresponding decoders to the list\n",
    "        count = 0\n",
    "        for fea_in, fea_out, pic_size, depth in zip(\n",
    "            arr_fea_in, arr_fea_out, arr_pic_size, arr_depth\n",
    "        ):\n",
    "            # Create Next Decoder (pay attention to special case in \"add_layers\" of DownUpConv)\n",
    "            cur_net = DownUpConv(\n",
    "                args,\n",
    "                n_fea_in=fea_in,\n",
    "                n_fea_next=fea_out,\n",
    "                pic_size=pic_size,\n",
    "                depth=depth,\n",
    "                move=\"up\",\n",
    "            )\n",
    "            # Append this to the list of networks\n",
    "            self.Dec.append(cur_net)\n",
    "\n",
    "        self.len = len(self.Enc)\n",
    "\n",
    "        # Finally the required Reshaping dimensions depending on 2D / 3D\n",
    "        if args.dim == 2:\n",
    "            # 2D Conv\n",
    "            self.reshape_q_pre = (0, 2, 3, 1)\n",
    "            self.reshape_q_pos = (0, 3, 1, 2)\n",
    "        else:\n",
    "            # 3D Conv\n",
    "            self.reshape_q_pre = (0, 2, 3, 4, 1)\n",
    "            self.reshape_q_pos = (0, 4, 1, 2, 3)\n",
    "\n",
    "    def classification(self, arr_q, update=True):\n",
    "        \"\"\"\n",
    "        Apply Classification on the quantization steps\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        res = 0\n",
    "\n",
    "        for cla, q in zip(self.Cla, reversed(arr_q)):\n",
    "            # concatenate the input\n",
    "            if count > 0:\n",
    "                q = torch.cat([res, q], 1)\n",
    "\n",
    "            # calculate output\n",
    "            res = cla(q)\n",
    "            count += 1\n",
    "\n",
    "        return res\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Use the Encoder Networks to encode all and save all steps in between\n",
    "        \"\"\"\n",
    "        # init empty array\n",
    "        arr_x = []\n",
    "        # Go over all Encoders\n",
    "        for enc in self.Enc:\n",
    "            x = enc(x)\n",
    "            arr_x.append(x)\n",
    "\n",
    "        return arr_x\n",
    "\n",
    "    def decode_quant(self, arr_x, update=True):\n",
    "        \"\"\"\n",
    "        Decode the input while performing quantizations:\n",
    "        Return Decoded Picture and the latent difference\n",
    "        \"\"\"\n",
    "        # Inits\n",
    "        count = 0\n",
    "        latent_diff = 0\n",
    "        q_i = 0\n",
    "        arr_q = []\n",
    "\n",
    "        # Quantizize and Decode\n",
    "        for dec, quant, x_i in zip(self.Dec, self.Quant, reversed(arr_x)):\n",
    "            # Concatenate the Arrays\n",
    "            if count > 0:\n",
    "                x_i = torch.cat([q_i, x_i], 1)\n",
    "\n",
    "            # Reformat for Quantization\n",
    "            x_i = x_i.permute(self.reshape_q_pre)\n",
    "            # Perform Quantization and update embeddings\n",
    "            q_i, diff_i, _ = quant(x_i, update)\n",
    "            # Reformat to old shape\n",
    "            q_i = q_i.permute(self.reshape_q_pos)\n",
    "            # add the quantization to the list of quantizations\n",
    "            if self.classify:\n",
    "                arr_q.append(q_i)\n",
    "            # Decode the quant result\n",
    "            q_i = dec(q_i)\n",
    "            # Increase count\n",
    "            count += 1\n",
    "            # Save latent losses\n",
    "            latent_diff += diff_i\n",
    "\n",
    "        # the output:\n",
    "        out = q_i\n",
    "\n",
    "        if self.classify:\n",
    "            return out, latent_diff, arr_q\n",
    "\n",
    "        return out, latent_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 128, 8, 8])\n",
      "torch.Size([20, 8, 8, 128])\n",
      "torch.Size([20, 8, 8, 128])\n",
      "torch.Size([20, 128, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# 2D Conv\n",
    "q_pre_2d = (0, 2, 3, 1)\n",
    "q_pos_2d = (0, 3, 1, 2)\n",
    "\n",
    "quant = Quantize(128, 512)\n",
    "inp2d = torch.randn(20, 128, 8, 8)\n",
    "print(inp2d.shape)\n",
    "inp2d = inp2d.permute(q_pre_2d)\n",
    "print(inp2d.shape)\n",
    "output, _, _ = quant(inp2d)\n",
    "print(output.shape)\n",
    "output = output.permute(q_pos_2d)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 128, 4, 8, 8])\n",
      "torch.Size([20, 4, 8, 8, 128])\n",
      "torch.Size([20, 4, 8, 8, 128])\n",
      "torch.Size([20, 128, 4, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# 3D Conv\n",
    "q_pre_3d = (0, 2, 3, 4, 1)\n",
    "q_pos_3d = (0, 4, 1, 2, 3)\n",
    "\n",
    "inp3d = torch.randn(20, 128, 4, 8, 8)\n",
    "print(inp3d.shape)\n",
    "inp3d = inp3d.permute(q_pre_3d)\n",
    "print(inp3d.shape)\n",
    "output, _, _ = quant(inp3d)\n",
    "print(output.shape)\n",
    "output = output.permute(q_pos_3d)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Complete VQVAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AbsModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f04587572e66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mVQVAE2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAbsModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m      6\u001b[0m     \u001b[0mVector\u001b[0m \u001b[0mQuantized\u001b[0m \u001b[0mVariational\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AbsModel' is not defined"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class VQVAE2(AbsModel):\n",
    "    \"\"\"\n",
    "    Vector Quantized Variational AutoEncoder\n",
    "    based on https://arxiv.org/abs/1906.00446\n",
    "    adapted from https://github.com/rosinality/vq-vae-2-pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, args):\n",
    "        \"\"\"Network and parameter definitions\"\"\"\n",
    "        super(VQVAE2, self).__init__(args)\n",
    "\n",
    "        # Initialise all networks within the Enc-Dec List\n",
    "        self.device = device\n",
    "        self.EncQuantDec = EncQuantDec(args).to(self.device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.vq_beta = args.vq_beta\n",
    "        self.optimizer = optim.Adam(self.EncQuantDec.parameters(), lr=args.lr)\n",
    "\n",
    "        # define forward function\n",
    "        self.forward = self.forward_normal\n",
    "\n",
    "        self.vq_class = 0\n",
    "\n",
    "        # choose the right forward function\n",
    "        if args.vq_classify:\n",
    "            # get the classifier specific part\n",
    "            self.class_criterion = nn.BCELoss()\n",
    "            self.y_labels = args.classes\n",
    "            self.y_len = len(self.y_labels)\n",
    "            self.forward = self.forward_class\n",
    "\n",
    "        if args.vq_gan:\n",
    "            # get the gan specific part\n",
    "            self.gan_loss = nn.BCELoss()\n",
    "            self.gan_real = 1\n",
    "            self.gan_fake = 0\n",
    "            self.forward = self.forward_gan\n",
    "            self.lam = args.lam\n",
    "\n",
    "            # get a single classifier:\n",
    "            # deactivate the model type to not have vq layers\n",
    "            args.model_type = \"\"\n",
    "            self.Cla = Discriminator(args, 1).to(self.device)\n",
    "            self.optimizer_Cla = optim.Adam(self.Cla.parameters(), lr=args.lr)\n",
    "            # reactivate model\n",
    "            args.model_type = \"vqvae\"\n",
    "\n",
    "    def set_parameters(self, args):\n",
    "        \"\"\"reset the intern parameters to allow pretraining\"\"\"\n",
    "        self.vq_class = args.vq_class\n",
    "\n",
    "    def calc_gradient_penalty(self, real_data, fake_data):\n",
    "        \"\"\"\n",
    "        Apply the gradient Penalty for Discriminator training\n",
    "        This is responsible for ensuring the Lipschitz constraint,\n",
    "        which is required to ensure the Wasserstein distance.\n",
    "        modified from: https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py \n",
    "        \"\"\"\n",
    "        # Asssign random factor alpha between 0 and 1\n",
    "        sh = real_data.shape\n",
    "        b_size = sh[0]\n",
    "        alpha = torch.rand(b_size, 1)\n",
    "        alpha = (\n",
    "            alpha.expand(b_size, int(real_data.nelement() / b_size))\n",
    "            .contiguous()\n",
    "            .view(sh)\n",
    "        )\n",
    "        alpha = alpha.to(self.device)\n",
    "\n",
    "        # interpolating as disc input\n",
    "        interpolates = (alpha * real_data + ((1 - alpha) * fake_data)).to(self.device)\n",
    "        interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "        # evaluate discriminator\n",
    "        disc_interpolates = self.Cla(interpolates)\n",
    "\n",
    "        # calculate gradients\n",
    "        gradients = autograd.grad(\n",
    "            outputs=disc_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones(disc_interpolates.size()).to(self.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "        # constrain gradients\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.lam\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    def forward_gan(self, data, update=True):\n",
    "        \"\"\"\n",
    "        Determin the training as GAN oriented:\n",
    "        The Classifier becomes a Dterminator as well\n",
    "        no sampling.. but reconstructed images are marked as fake\n",
    "        \"\"\"\n",
    "        # Move img-input on GPU\n",
    "        inp = data[\"img\"].to(self.device)\n",
    "\n",
    "        # (1) Train Dicriminator\n",
    "        # 1.1 Train with all-real batch\n",
    "        self.Cla.zero_grad()\n",
    "        output = self.Cla(inp).view(-1)\n",
    "        errD_real = -output.mean()\n",
    "        # store output\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # 1.2 Train with all-fake batch\n",
    "        # build fake:\n",
    "        arr_x = self.EncQuantDec.encode(inp)\n",
    "        arr_x = [ele.detach() for ele in arr_x]\n",
    "        x_r, latent_diff = self.EncQuantDec.decode_quant(arr_x, update=False)\n",
    "        output = self.Cla(x_r.detach()).view(-1)\n",
    "        errD_fake = output.mean()\n",
    "        # store output\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        gradient_penalty = self.calc_gradient_penalty(inp, x_r.detach())\n",
    "        errD = errD_fake + errD_real + gradient_penalty\n",
    "\n",
    "        if update:\n",
    "            errD.backward()\n",
    "            self.optimizer_Cla.step()\n",
    "\n",
    "        # (2) Train Encoder Quant / Discriminator\n",
    "        self.EncQuantDec.zero_grad()\n",
    "        # Gan part\n",
    "        output = self.Cla(x_r).view(-1)\n",
    "        errG = -output.mean().tanh()\n",
    "        if update and self.vq_class > 0:\n",
    "            errG.backward()\n",
    "\n",
    "        # vqvae part\n",
    "        arr_x = self.EncQuantDec.encode(inp)\n",
    "        x_r, latent_diff = self.EncQuantDec.decode_quant(arr_x, update=True)\n",
    "        recon_loss = self.criterion(x_r, inp)\n",
    "        latent_loss = latent_diff.mean()\n",
    "\n",
    "        # final loss\n",
    "        loss = recon_loss + self.vq_beta * latent_loss\n",
    "\n",
    "        # Update Generator\n",
    "        if update:\n",
    "            loss.backward()\n",
    "            # composed of Enc/Dec/Quant\n",
    "            self.optimizer.step()\n",
    "\n",
    "        else:\n",
    "            # Track all relevant losses\n",
    "            tr_data = {}\n",
    "            tr_data[\"l_all\"] = loss.item()\n",
    "            tr_data[\"l_dis\"] = errD.item()\n",
    "            tr_data[\"l_gen\"] = errG.item()\n",
    "            tr_data[\"l_recon\"] = recon_loss.item()\n",
    "            tr_data[\"l_latent\"] = latent_loss.item()\n",
    "            tr_data[\"D_real(1)\"] = D_x\n",
    "            tr_data[\"D_fake(0)\"] = D_G_z1\n",
    "            # Return losses and fake data\n",
    "            return x_r.detach(), tr_data\n",
    "\n",
    "    def forward_class(self, data, update=True):\n",
    "        \"\"\"\n",
    "        With included classification\n",
    "        Encode-Quantize-Decode and update\n",
    "        \"\"\"\n",
    "        # Move img-input on GPU\n",
    "        inp = self.prep(data).to(self.device)\n",
    "        # Reset Gradients\n",
    "        self.EncQuantDec.zero_grad()\n",
    "        # Encode\n",
    "        arr_x = self.EncQuantDec.encode(inp)\n",
    "        # Decode and Quantizice - update Embeddings\n",
    "        x_re, latent_diff, arr_q = self.EncQuantDec.decode_quant(arr_x, update)\n",
    "        # append x_re to arr_q\n",
    "        arr_q.append(x_re)\n",
    "        # Classify\n",
    "        res = self.EncQuantDec.classification(arr_q, update)\n",
    "        # Get the true labels\n",
    "        y = torch.zeros(inp.shape[0], self.y_len)  # init\n",
    "        for i, cl in enumerate(self.y_labels):\n",
    "            y[:, i] = data[cl]  # fill\n",
    "        y = y.to(self.device)  # send to device\n",
    "        # get the classification loss\n",
    "        class_loss = self.class_criterion(res, y)\n",
    "\n",
    "        # Calculate the reconstruction loss\n",
    "        recon_loss = self.criterion(x_re, inp)\n",
    "        # Calculate the latent loss\n",
    "        latent_loss = latent_diff.mean()\n",
    "\n",
    "        # Get the final loss\n",
    "        loss = recon_loss + self.vq_beta * latent_loss + self.vq_class * class_loss\n",
    "\n",
    "        # Backpropagate and Update:\n",
    "        if update:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # Return the output\n",
    "            return x_re\n",
    "\n",
    "        # Return a dictionary of data to track\n",
    "        else:\n",
    "            tr_data = {}\n",
    "            tr_data[\"l_all\"] = loss.item()\n",
    "            tr_data[\"l_class\"] = class_loss.item()\n",
    "            tr_data[\"l_recon\"] = recon_loss.item()\n",
    "            tr_data[\"l_latent\"] = latent_loss.item()\n",
    "\n",
    "            # Return output and losses\n",
    "            return x_re, tr_data\n",
    "\n",
    "    def forward_normal(self, data, update=True):\n",
    "        \"\"\"Encode-Quantize-Decode and update\"\"\"\n",
    "        # Move img-input on GPU\n",
    "        inp = self.prep(data).to(self.device)\n",
    "        # Reset Gradients\n",
    "        self.EncQuantDec.zero_grad()\n",
    "        # Encode\n",
    "        arr_x = self.EncQuantDec.encode(inp)\n",
    "        # Decode and Quantizice - update Embeddings\n",
    "        x_re, latent_diff = self.EncQuantDec.decode_quant(arr_x, update)\n",
    "        # Calculate the reconstruction loss\n",
    "        recon_loss = self.criterion(x_re, inp)\n",
    "        # Calculate the latent loss\n",
    "        latent_loss = latent_diff.mean()\n",
    "        # Get the final loss\n",
    "        loss = recon_loss + self.vq_beta * latent_loss\n",
    "\n",
    "        # Backpropagate and Update:\n",
    "        if update:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Return the output\n",
    "            return x_re.detach()\n",
    "\n",
    "        # Return a dictionary of data to track\n",
    "        else:\n",
    "            tr_data = {}\n",
    "            tr_data[\"l_all\"] = loss.item()\n",
    "            tr_data[\"l_recon\"] = recon_loss.item()\n",
    "            tr_data[\"l_latent\"] = latent_loss.item()\n",
    "\n",
    "            # Return output and losses\n",
    "            return x_re.detach(), tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from deeptool.train_loop import test_one_batch\n",
    "from deeptool.parameters import get_all_args, compat_args\n",
    "\n",
    "args = get_all_args()\n",
    "args.pic_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqvae\n",
      "<class 'deeptool.model.vqvae.VQVAE2'>\n"
     ]
    }
   ],
   "source": [
    "# 3 dim test\n",
    "args.model_type = \"vqvae\"\n",
    "args.dim = 3\n",
    "args = compat_args(args)\n",
    "test_one_batch(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqvae\n",
      "<class 'deeptool.model.vqvae.VQVAE2'>\n"
     ]
    }
   ],
   "source": [
    "# 2 dim test\n",
    "args.dim = 2\n",
    "args = compat_args(args)\n",
    "test_one_batch(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataloader.ipynb.\n",
      "Converted 01_architecture.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_parameters.ipynb.\n",
      "Converted 04_train_loop.ipynb.\n",
      "Converted 05_abstract_model.ipynb.\n",
      "Converted 10_diagnosis.ipynb.\n",
      "Converted 20_dcgan.ipynb.\n",
      "Converted 21_introvae.ipynb.\n",
      "Converted 22_vqvae.ipynb.\n",
      "Converted 23_bigan.ipynb.\n",
      "Converted 24_mocoae.ipynb.\n",
      "Converted 33_rnn_vae.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
