{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.simsiamae\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimSiam Autoencoder\n",
    "\n",
    "> Simple Siamese Loss for Autoencoder based Representation Learning\n",
    "> Based on the paper: https://arxiv.org/abs/2011.10566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from deeptool.architecture import Encoder, Decoder, DownUpConv\n",
    "from deeptool.abs_model import AbsModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], dtype=torch.float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# deal with varying list and batch sizes:\n",
    "\n",
    "a = list(range(100))\n",
    "ptr = 99\n",
    "batch_size = 4\n",
    "K = 100\n",
    "\n",
    "ind1 = list(range((ptr + batch_size) % K))\n",
    "ind2 = list(range(ptr, K))\n",
    "indexes = ind1 + ind2\n",
    "indexes\n",
    "torch.tensor(np.array([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, in_dim=2048, hidden_dim=512, out_dim=2048): # bottleneck structure\n",
    "        super(Predictor, self).__init__()\n",
    "        ''' page 3 baseline setting\n",
    "        Prediction MLP. The prediction MLP (h) has BN applied \n",
    "        to its hidden fc layers. Its output fc does not have BN\n",
    "        (ablation in Sec. 4.4) or ReLU. This MLP has 2 layers. \n",
    "        The dimension of h’s input and output (z and p) is d = 2048, \n",
    "        and h’s hidden layer’s dimension is 512, making h a \n",
    "        bottleneck structure (ablation in supplement). \n",
    "        '''\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \"\"\"\n",
    "        Adding BN to the output of the prediction MLP h does not work\n",
    "        well (Table 3d). We find that this is not about collapsing. \n",
    "        The training is unstable and the loss oscillates.\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class SimSiamAE(AbsModel):\n",
    "    \"\"\"\n",
    "    The SimSiam contains the Autoencoder based Architecture and the modified Pretext task\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, args):\n",
    "        \"\"\"init the network\"\"\"\n",
    "        super(SimSiamAE, self).__init__(args)\n",
    "        self.device = device  # GPU\n",
    "        self.dim = args.dim   # 2/3 Dimensional input\n",
    "        self.n_z = args.n_z   # Compression\n",
    "        self.ae_mode = args.moco_aemode # inerited from moco\n",
    "\n",
    "        # Encoder\n",
    "        self.enc = Encoder(args, vae_mode=False).to(self.device)  # encoder\n",
    "        \n",
    "        # Predictor\n",
    "        self.pred = Predictor(self.n_z, int(0.25 * self.n_z), self.n_z).to(self.device)\n",
    "        \n",
    "        # optimizers\n",
    "        self.optimizerEnc = optim.SGD(self.enc.parameters(), lr=args.lr)\n",
    "        self.optimizerPred = optim.SGD(self.pred.parameters(), lr=args.lr)\n",
    "        \n",
    "        # override prep and take\n",
    "        self.prep = self.prep_3D if args.dataset_type == \"MRNet\" else self.prep_2D\n",
    "        self.take = self.take_3D if args.dataset_type == \"MRNet\" else self.take_2D\n",
    "        \n",
    "        # init the decoder\n",
    "        self.init_ae(args)\n",
    "    \n",
    "    def init_ae(self, args):\n",
    "        \"\"\"Init the Autoencoder specific parts\"\"\"\n",
    "        # Decoder\n",
    "        self.init_dec(args)\n",
    "        \n",
    "        # loss function\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def init_dec(self, args):\n",
    "        \"\"\"Init a general decoder\"\"\"\n",
    "        self.dec = Decoder(args).to(self.device)  # decoder\n",
    "        self.optimizerDec = optim.Adam(self.dec.parameters(), lr=args.lr)\n",
    "    \n",
    "    def prep_2D(self, data):\n",
    "        return data[0][0]\n",
    "    \n",
    "    def prep_3D(self, data, key=\"img\"):\n",
    "        return data[key]\n",
    "    \n",
    "    def take_2D(self, data):\n",
    "        return data[0][0], data[0][1]\n",
    "    \n",
    "    def take_3D(self, data, key=\"img\"):\n",
    "        return data[key], data[key]\n",
    "\n",
    "    def ae_forward(self, x, update):\n",
    "        \"\"\"\n",
    "        Classic regression part of a normal Autoencoder\n",
    "        \"\"\"\n",
    "        x_r = self.dec(z)\n",
    "        ae_loss = self.mse_loss(x_r, x)\n",
    "        \n",
    "        return x_r, ae_loss\n",
    "    \n",
    "    def D(self, p, z):\n",
    "        \"\"\"\n",
    "        negative cosine similarity\n",
    "        \"\"\"\n",
    "        # stop gradient on z\n",
    "        z = z.detach()\n",
    "        \n",
    "        # normalize\n",
    "        p = nn.functional.normalize(p, dim=1)\n",
    "        z = nn.functional.normalize(z, dim=1)\n",
    "        \n",
    "        return -(p * z).sum(dim=-1).mean()\n",
    "    \n",
    "    def forward(self, data, update=True):\n",
    "        # Reset Gradients\n",
    "        self.optimizerEnc.zero_grad()\n",
    "        self.optimizerPred.zero_grad()\n",
    "        self.optimizerDec.zero_grad()\n",
    "        \n",
    "        # 1. Get the augmented data\n",
    "        x1, x2 = self.take(data)\n",
    "\n",
    "        # 2. Send pictures to device\n",
    "        x1 = x1.to(self.device)\n",
    "        x2 = x2.to(self.device)\n",
    "        \n",
    "        # 3. Encode\n",
    "        z1 = self.enc(x1)\n",
    "        z2 = self.enc(x2)\n",
    "        \n",
    "        # 4. Predictor\n",
    "        p1 = self.pred(z1)\n",
    "        p2 = self.pred(z2)\n",
    "        \n",
    "        # 5. Decode\n",
    "        x3 = self.dec(z1)\n",
    "        \n",
    "        # 6. Encode again\n",
    "        z3 = self.enc(x3.detach())\n",
    "        \n",
    "        # 7. Predict again\n",
    "        p3 = self.pred(z3)\n",
    "        \n",
    "        # 8. define losses\n",
    "        l_all = 0\n",
    "        l_ae = self.mse_loss(x1, x3) * 2\n",
    "        l_d1 = self.D(p1, z2) / 2 + self.D(p2, z1) / 2\n",
    "        l_d2 = self.D(p1, z3) / 2 + self.D(p3, z1) / 2\n",
    "        l_all += (l_d1 + l_d2 + l_ae)\n",
    "        \n",
    "        # 9. Perform update\n",
    "        if update:\n",
    "            l_all.backward()\n",
    "            \n",
    "            # Apply gradient step\n",
    "            self.optimizerEnc.step()\n",
    "            self.optimizerPred.step()\n",
    "            self.optimizerDec.step()\n",
    "            return x3.detach()\n",
    "        \n",
    "        else:\n",
    "            tr_data = {\n",
    "                \"loss_ae\": l_ae,\n",
    "                \"loss_D1\": l_d1,\n",
    "                \"loss_D2\": l_d2,\n",
    "                \"l_all\": l_all,\n",
    "            }\n",
    "            return x3.detach(), tr_data\n",
    "        \n",
    "\n",
    "    def forward_old(self, data, update=True):\n",
    "        \"\"\"\n",
    "        Perform forward computation and update\n",
    "        \"\"\"\n",
    "        # Reset Gradients\n",
    "        self.optimizerEnc.zero_grad()\n",
    "        self.optimizerPred.zero_grad()\n",
    "        self.optimizerDec.zero_grad() if self.ae_mode else None\n",
    "\n",
    "        # 1. Get the augmented data\n",
    "        x1, x2 = self.take(data)\n",
    "\n",
    "        # 2. Send pictures to device\n",
    "        x1 = x1.to(self.device)\n",
    "        x2 = x2.to(self.device)\n",
    "        \n",
    "        # 3. Encode\n",
    "        z1 = self.enc(x1)\n",
    "        z2 = self.enc(x2)\n",
    "        \n",
    "        # 4. Predictor\n",
    "        p1 = self.pred(z1)\n",
    "        p2 = self.pred(z2)\n",
    "        \n",
    "        l_all = 0\n",
    "        l_d = self.D(p1, z2) / 2 + self.D(p2, z1) / 2\n",
    "        l_all += l_d\n",
    "        \n",
    "        # Add the reconstruction loss\n",
    "        if self.ae_mode:\n",
    "            xr = self.dec(z1)\n",
    "            l_ae = self.mse_loss(xr, x1)\n",
    "            l_all += l_ae\n",
    "\n",
    "        # Perform encoder update\n",
    "        if update:\n",
    "            l_all.backward()\n",
    "            \n",
    "            # Optimizers\n",
    "            self.optimizerEnc.step()\n",
    "            self.optimizerPred.step()\n",
    "            self.optimizerDec.step() if self.ae_mode else None\n",
    "            return xr.detach()\n",
    "        \n",
    "        else:\n",
    "            tr_data = {\n",
    "                \"loss_ae\": l_ae,\n",
    "                \"loss_D\": l_d,\n",
    "                \"l_all\": l_all,\n",
    "            }\n",
    "            return xr.detach(), tr_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
