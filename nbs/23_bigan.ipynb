{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.bigan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from deeptool.architecture import Encoder, Decoder, DownUpConv, weights_init\n",
    "from deeptool.abs_model import AbsModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiGAN\n",
    "\n",
    "> its bigan time ladies and gentleman!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bigan.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "     \n",
    "https://arxiv.org/abs/1907.02544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class DisBiGan(nn.Module):\n",
    "    \"\"\"\n",
    "    The redefined Discriminator for Bigan:\n",
    "    Contains the classic discriminator and concatenates the input with the hiddem dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"The Discriminator for BiGan: will include Conv and fully part\"\"\"\n",
    "        super(DisBiGan, self).__init__()\n",
    "\n",
    "        # convolutional neural network\n",
    "        self.conv_part = DownUpConv(\n",
    "            args,\n",
    "            n_fea_next=args.n_fea_down,\n",
    "            move=\"down\",\n",
    "            pic_size=args.pic_size,\n",
    "            depth=args.crop_size,\n",
    "            n_fea_in=len(args.perspectives),\n",
    "            p_drop=args.p_drop,\n",
    "        )\n",
    "        self.max_fea = self.conv_part.max_fea\n",
    "        self.hidden_dim = self.max_fea * args.min_size ** (args.dim)\n",
    "        self.last_dim = int(args.n_z / 2)\n",
    "\n",
    "        # Finish with fully connected layers\n",
    "        self.fc_part_sxz = nn.Sequential(\n",
    "            # State size batch x (cur_fea*4*4*4)\n",
    "            nn.Linear(self.hidden_dim + args.n_z, self.last_dim, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=args.p_drop),\n",
    "            # Output size batch x n_z\n",
    "            nn.Linear(self.last_dim, 1, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "            # Output size batch x 1\n",
    "        )\n",
    "        self.fc_part_sxz.apply(weights_init)\n",
    "\n",
    "        self.forward = self.forward_normal\n",
    "        # add two further fully connected parts if the extension is set to \"TRUE\"\n",
    "        if args.bi_extension:\n",
    "            self.forward = self.forward_extension\n",
    "            # picture extension\n",
    "            self.fc_part_sx = nn.Sequential(\n",
    "                # State size batch x (cur_fea*4*4*4)\n",
    "                nn.Linear(self.hidden_dim, self.last_dim, bias=False),\n",
    "                nn.Dropout(p=args.p_drop),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                # Output size batch x n_z\n",
    "                nn.Linear(self.last_dim, 1, bias=False),\n",
    "                nn.Sigmoid(),\n",
    "                # Output size batch x 1\n",
    "            )\n",
    "            self.fc_part_sx.apply(weights_init)\n",
    "\n",
    "            # latent extension\n",
    "            self.fc_part_sz = nn.Sequential(\n",
    "                # State size batch x (cur_fea*4*4*4)\n",
    "                nn.Linear(args.n_z, self.last_dim, bias=False),\n",
    "                nn.Dropout(p=args.p_drop),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                # Output size batch x n_z\n",
    "                nn.Linear(self.last_dim, 1, bias=False),\n",
    "                nn.Sigmoid(),\n",
    "                # Output size batch x 1\n",
    "            )\n",
    "            self.fc_part_sz.apply(weights_init)\n",
    "\n",
    "    def forward_normal(self, inp):\n",
    "        \"\"\"\n",
    "        Perform forward calculation\n",
    "        input is tuple of: picture x and laten z\n",
    "        \"\"\"\n",
    "        x, z = inp\n",
    "        # first apply convolutions on picture x\n",
    "        x = self.conv_part(x)\n",
    "        # Resize\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        # now concatenate to get the extend dimension\n",
    "        x = torch.cat([x, z], dim=1)\n",
    "        # now apply the fully connected part on these\n",
    "        x = self.fc_part_sxz(x)\n",
    "        # return the determined value\n",
    "        return x\n",
    "\n",
    "    def forward_extension(self, inp):\n",
    "        \"\"\"\n",
    "        Perform forward calculation\n",
    "        input is tuple of: picture x and laten z\n",
    "        \"\"\"\n",
    "        x, z = inp\n",
    "        x = self.conv_part(x)\n",
    "        x = x.view((-1, self.hidden_dim))\n",
    "        # get the value for sxz\n",
    "        s_xz = torch.cat([x, z], dim=1)\n",
    "        s_xz = self.fc_part_sxz(x)\n",
    "        # get the value for sx\n",
    "        s_x = self.fc_part_sx(x)\n",
    "        # get the value for sz\n",
    "        s_z = self.fc_part_sx(z)\n",
    "        return s_xz, s_x, s_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class BiGAN(AbsModel):\n",
    "    \"\"\"\n",
    "    The Bidirectional Generative adversarial network\n",
    "    based on https://arxiv.org/abs/1605.09782\n",
    "    extension based on: https://arxiv.org/abs/1907.02544\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, args):\n",
    "        \"\"\"\n",
    "        network architecture\n",
    "        \"\"\"\n",
    "        super(BiGAN, self).__init__(args)\n",
    "        self.device = device\n",
    "        self.dim = args.dim\n",
    "        self.n_z = args.n_z\n",
    "\n",
    "        # Loss to be optimized for dcgan\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "        # labeling\n",
    "        self.real_label = 1\n",
    "        self.fake_label = 0\n",
    "\n",
    "        # the three relevant networks\n",
    "        self.decoder = Decoder(args).to(self.device)\n",
    "        self.encoder = Encoder(args, vae_mode=False).to(self.device)\n",
    "        self.discriminator = DisBiGan(args).to(self.device)\n",
    "\n",
    "        # parameters\n",
    "        self.lam = args.lam\n",
    "\n",
    "        # the optimizers\n",
    "        self.optimizerDec = optim.Adam(\n",
    "            self.decoder.parameters(), lr=args.lr, betas=(0.5, 0.999)\n",
    "        )\n",
    "        self.optimizerEnc = optim.Adam(\n",
    "            self.encoder.parameters(), lr=args.lr, betas=(0.5, 0.999)\n",
    "        )\n",
    "        self.optimizerDis = optim.Adam(\n",
    "            self.discriminator.parameters(), lr=args.lr, betas=(0.5, 0.999)\n",
    "        )\n",
    "\n",
    "        # Fixed noise to visualize progression\n",
    "        self.batch_size = args.batch_size\n",
    "        self.fixed_noise = torch.randn(self.batch_size, self.n_z, device=self.device)\n",
    "\n",
    "    def sample_noise(self, batch_size, update):\n",
    "        \"\"\"\n",
    "        Sample the latent noise required for training\n",
    "        \"\"\"\n",
    "        if update:\n",
    "            return torch.randn(batch_size, self.n_z, device=self.device)\n",
    "        return self.fixed_noise\n",
    "\n",
    "    def calc_gradient_penalty(self, real, fake):\n",
    "        \"\"\"\n",
    "        Apply the gradient Penalty for Discriminator training\n",
    "        This is responsible for ensuring the Lipschitz constraint,\n",
    "        which is required to ensure the Wasserstein distance.\n",
    "        modified from: https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py \n",
    "        \"\"\"\n",
    "        # Asssign random factor alpha between 0 and 1\n",
    "        sh = real.shape\n",
    "        b_size = sh[0]\n",
    "        alpha = torch.rand(b_size, 1)\n",
    "        alpha = (\n",
    "            alpha.expand(b_size, int(real.nelement() / b_size)).contiguous().view(sh)\n",
    "        )\n",
    "        alpha = alpha.to(self.device)\n",
    "\n",
    "        # interpolating as disc input\n",
    "        interpolates = (alpha * real + ((1 - alpha) * fake)).to(self.device)\n",
    "        interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "        # evaluate discriminator\n",
    "        disc_interpolates = self.discriminator(interpolates)\n",
    "\n",
    "        # calculate gradients\n",
    "        gradients = autograd.grad(\n",
    "            outputs=disc_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones(disc_interpolates.size()).to(self.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "        # constrain gradients\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.lam\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    def forward(self, data, update=True):\n",
    "        \"\"\"\n",
    "        Calculate output and update networks with dcgan\n",
    "        \"\"\"\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # ------------------------------------------------------------\n",
    "        # 1.1 Train with all-real batch\n",
    "        # Get the true data\n",
    "        real_x = self.prep(data).to(self.device)\n",
    "        real_z = self.encoder(real_x)\n",
    "        real = (real_x, real_z.detach())\n",
    "\n",
    "        self.discriminator.zero_grad()\n",
    "\n",
    "        # fill the labels\n",
    "        b_size = real_x.size(0)\n",
    "        label = torch.full((b_size,), self.real_label, device=self.device)\n",
    "\n",
    "        # True set\n",
    "        output = self.discriminator(real).view(-1)\n",
    "        label.fill_(self.real_label)\n",
    "        errD_real = self.loss(output, label)\n",
    "        errD_real.backward() if update else None\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # 1.2 Train with all-fake batch\n",
    "        fake_z = self.sample_noise(b_size, update)\n",
    "        fake_x = self.decoder(fake_z)\n",
    "        fake = (fake_x.detach(), fake_z.detach())\n",
    "\n",
    "        # Fake set\n",
    "        output = self.discriminator(fake).view(-1)\n",
    "        label.fill_(self.fake_label)\n",
    "        errD_fake = self.loss(output, label)\n",
    "        errD_fake.backward() if update else None\n",
    "        \n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        # final discriminatro loss\n",
    "        errD = errD_fake.item() + errD_real.item()\n",
    "\n",
    "        # Update Discriminator\n",
    "        if update:\n",
    "            self.optimizerDis.step()\n",
    "\n",
    "        # (2) Update G network: maximize 1 - log(D(G(z)))\n",
    "        # ------------------------------------------------------------\n",
    "        self.encoder.zero_grad()\n",
    "        self.decoder.zero_grad()\n",
    "\n",
    "        # True set\n",
    "        real = (real_x, real_z)\n",
    "        output = self.discriminator(real).view(-1)\n",
    "        label.fill_(self.fake_label)\n",
    "        errE = self.loss(output, label)\n",
    "        errE.backward() if update else None\n",
    "\n",
    "        # Fake set\n",
    "        fake = (fake_x, fake_z)\n",
    "        output = self.discriminator(fake).view(-1)\n",
    "        label.fill_(self.real_label)\n",
    "        errD = self.loss(output, label)\n",
    "        errD.backward() if update else None\n",
    "\n",
    "        # Update Generator\n",
    "        if update:\n",
    "            self.optimizerEnc.step()\n",
    "            self.optimizerDec.step()\n",
    "            return fake_x\n",
    "\n",
    "        else:\n",
    "            # Track all relevant losses\n",
    "            tr_data = {}\n",
    "            tr_data[\"errD\"] = errD\n",
    "            tr_data[\"errG\"] = errD.item()\n",
    "            tr_data[\"D_x\"] = D_x\n",
    "            tr_data[\"D_G_z1\"] = D_G_z1\n",
    "            tr_data[\"D_G_z2\"] = output.mean().item()\n",
    "\n",
    "            # generate the autoencoder output:\n",
    "            x_r = self.decoder(real_z)\n",
    "\n",
    "            # Return losses and reconstruction data\n",
    "            return x_r, tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from deeptool.train_loop import test_one_batch\n",
    "from deeptool.parameters import get_all_args, compat_args\n",
    "\n",
    "args = get_all_args()\n",
    "args.pic_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigan\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# 3 dim test\n",
    "args.model_type = \"bigan\"\n",
    "args.dim = 3\n",
    "args = compat_args(args)\n",
    "test_one_batch(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigan\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# 2 dim test\n",
    "args.model_type = \"bigan\"\n",
    "args.dim = 2\n",
    "args = compat_args(args)\n",
    "test_one_batch(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataloader.ipynb.\n",
      "Converted 01_architecture.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_parameters.ipynb.\n",
      "Converted 04_train_loop.ipynb.\n",
      "Converted 05_abstract_model.ipynb.\n",
      "Converted 10_diagnosis.ipynb.\n",
      "Converted 20_dcgan.ipynb.\n",
      "Converted 21_introvae.ipynb.\n",
      "Converted 22_vqvae.ipynb.\n",
      "Converted 23_bigan.ipynb.\n",
      "Converted 24_mocoae.ipynb.\n",
      "Converted 33_rnn_vae.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
